\documentclass[11pt]{article}

\usepackage{comment} % enables the use of multi-line comments (\ifx \fi) 
\usepackage[a4paper,margin=1cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage[ngerman]{isodate}
\usepackage{gensymb}
\usepackage{graphicx}
\usepackage{booktabs}% http://ctan.org/pkg/booktabs
\usepackage{tabularx}
\usepackage{ltablex} % Longtables with tabularx
\usepackage[x11names]{xcolor}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{array}
\usepackage{wrapfig}
\usepackage{subcaption}
\usepackage{csquotes}
\usepackage{lscape}
\usepackage{geometry}
\usepackage{multicol}
\usepackage{bm}
\usepackage{enumitem}
\usepackage{hyperref}
\usepackage{mdframed}
\usepackage{scalerel}
\usepackage{stackengine}
\usepackage{mathtools}
\usepackage{pdfpages}

% Code highlighting
\usepackage{minted}
\surroundwithmdframed{minted}

% Be able to caption equations and float them in place
\usepackage{float}

\newmdtheoremenv{theorem}{Theorem}

\theoremstyle{definition}
\newmdtheoremenv{definition}{Definition}[section]


\geometry{a4paper, margin=2.4cm}

\newcommand\equalhat{\mathrel{\stackon[1.5pt]{=}{\stretchto{\scalerel*[\widthof{=}]{\wedge}{\rule{1ex}{3ex}}}{0.5ex}}}}
\newcommand\defeq{\mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny def}}}{=}}}
\newcolumntype{C}{>{\centering\arraybackslash}X}

\newcommand*\samplemean[1]{\overline{#1}}
\newcommand*\ev[1]{\mathrel{\text{E}\left[#1\right]}}
\newcommand*\R{\mathbb{R}}
\newcommand*\Z{\mathbb{Z}}
\newcommand*\N[1]{\mathcal{N}\left(#1\right)}
\newcommand*\Likelihood{\mathcal{L}}
\newcommand*\diff{\mathop{}\!\mathrm{d}}
\newcommand*\Diff[1]{\mathop{}\!\mathrm{d^#1}}
\newcommand*\Exp[1]{\mathop{\text{Exp}}\left(#1\right)}
\newcommand*\Cov[1]{\mathop{\text{Cov}}\left(#1\right)}
\newcommand*\Cor[1]{\mathop{\text{Cor}}\left(#1\right)}
\newcommand*\Var[1]{\mathop{\text{Var}}\left(#1\right)}
\newcommand*\se[1]{\mathop{\text{se}}\left(#1\right)}


\DeclarePairedDelimiter\abs{\lvert}{\rvert}
\DeclarePairedDelimiter\norm{\lVert}{\rVert}

\setcounter{tocdepth}{3}
\setcounter{secnumdepth}{3}

\graphicspath{{./img/}}

\begin{document}
	
\title{Applied Statistics and Data Analysis HS20}
\author{Pascal Baumann\\pascal.baumann@stud.hslu.ch}
\maketitle



For errors or improvement raise an issue or make a pull request on the \href{https://github.com/KilnOfTheSecondFlame/mse_summaries}{github repository}.

\tableofcontents
\newpage

\section{Introduction}
This is module concerns itself with quality control, especially with \textbf{statistical quality control}. Statistical process control is understood to be

\begin{definition}
	Statistical process control is, first and foremost, a \textbf{way of thinking}, which happens to have some tools attached.
\end{definition}

\section{Statistical Process and Quality Control}
These tools are seven statistical methods for analysing data colloquially known as \textbf{magnificent seven}:
\begin{enumerate}[noitemsep]
	\item Histogram
	\item Check Sheet
	\item Pareto Chart
	\item Defect Concentration diagram
	\item Cause-and-Effect Diagram
	\item Control Chart
	\item Scatter Diagram
\end{enumerate}

\subsubsection{Check Sheet}
The check sheet is a simple method of quality control and consists of a form for  registering and counting possible problems in a production process.

\subsubsection{Pareto Chart}
The Pareto chart is the application of the Pareto principle which states that \emph{80 percent of the result can be achieved with 20 percent of the commitment}. The numbers are arbitrarily chosen, but show up in this magnitude in many real world applications.

\subsection{Location Plot}
The location plot is also called the defect concentration diagram, and is used to graphically visualise the locations of various defects on the physical object.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/bullet_holes_wwii}
	\caption{Bullet holes found in WWII allied bombers which returned after sorties}
	\label{fig:bulletholeswwii}
\end{figure}

\subsubsection{Cause-and-Effect Diagram}
This tool has the form of a fishbone diagram and is used for the systematic identification of causes resulting in problems.

\subsection{Control Charts}
The basis for control charts is the idea that a process can run according to specifications or outside of them. These are upper and lower bound and are derived statistically. The process is then continuously monitored and plotted against these bounds.

\subsubsection{Construction of Control Charts}
Periodic sampling of fixed sample size $n$ from the current production of the machine. The new parts are then drawn in a scatter plot with a centreline $\mu_0$ and with upper and lower control limit for a given $\alpha$ which defines the \textbf{control area}. The mean values are then continuously plotted against the index of the current measurement in the scatter diagram. The problem is that the process standard deviation is unknown.

The distance between upper control limit (UCL) and lower control limit (LCL) is three times the standard deviation.
\begin{align*}
	\text{LCL} &= \mu_0 - z_q \frac{\sigma}{\sqrt{n}}\\
	\text{UCL} &= \mu_0 + z_q \frac{\sigma}{\sqrt{n}}
\end{align*}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/control_chart}
	\caption{Sample control chart with UCL and LCL indicated}
	\label{fig:controlchart}
\end{figure}

\subsubsection{Shewhart Control Charts}
The problem with constructing the control chart is that mean and standard deviation of a process are generally unknown. Shewhart solves this by first monitoring the variation and then the mean of a process. Sample the process periodically under the assumption that the process is normally distributed.
\begin{enumerate}
	\item Calculate the mean of each sample
	\begin{equation*}
		\samplemean{x}_i = \frac{1}{n}\sum_{j=1}^{n}x_{ij}
	\end{equation*}
	\item Calculate standard deviation of the samples
	\begin{equation*}
		s_i = \sqrt{\frac{1}{n-1} \sum_{j=1}^{n}(x_{ij}-\samplemean{x}_i)^2}
	\end{equation*}
	\item Calculate the range R for each sample
	\begin{equation*}
		R_i = \max(x_{ij}) - \min(x_{ij}) \qquad j\in1..n
	\end{equation*}
\end{enumerate}

\paragraph{Construction of an $R$ chart} Take the data from the trial run with $k$ samples of sample size $n$ from the process under supervision. The centreline (CL) of the process is denoted by $\samplemean{R}$ and calculated as follows
\begin{equation*}
	\samplemean{R} = \frac{1}{k}\sum_{i=1}^{k}R_i
\end{equation*}
The standard deviation of the statistic $\samplemean{R}$ is denoted $\sigma_R$. The control limits of the $R$ chart can then be calculated
\begin{align}
	\text{LCL} &= D_3\samplemean{R}\\
	\text{UCL} &= D_4\samplemean{R}\\
\end{align}
The constants $D_3$ and $D_4$ are dependent on the sample size $n$ and must be looked up in a specific table.

After the centreline and the control limits are determined the ranges $R_i$ are sequentially entered against the index $i$ in a scatter plot. In addition, lines for the control limits UCL and LCL are drawn into the same graph. If a sample is outside the control area, the sample is omitted and the limits recalculated.

\paragraph{Construction of an $\samplemean{x}$ chart} The control limits of an $\samplemean{x}$ chart are based on the three sigma limits of the empirical distribution of the statistic $\samplemean{x}$.

The control limits are then
\begin{align}
	\text{LCL} &= \mu - 3 \frac{\sigma}{\sqrt{n}}\\
	\text{UCL} &= \mu + 3 \frac{\sigma}{\sqrt{n}}
\end{align}
The assumption is that the $R$ chart is under statistical control and the real value of $\sigma$ can thus be approximated by
\begin{equation*}
	\hat{\sigma} = \frac{\samplemean{R}}{d_2}
\end{equation*}
as it is a reliable estimate for the process deviation. The constant $d_2$ is again dependent on the sample size $n$ and can be found in the table.

Any samples excluded for the construction of the $R$ chart, should also be disregarded for the construction of the $\samplemean{x}$ chart. This results in a data set of $k^\star$ valid samples and a corrected estimate for $\mu$
\begin{equation*}
	\samplemean{\samplemean{x}} = \frac{1}{k^\star}\sum_{i=1}^{k^\star}\samplemean{x}_i
\end{equation*}
and the corrected control limits
\begin{align*}
	\text{LCL} &= \samplemean{\samplemean{x}} - 3 \frac{\samplemean{R}}{d_2}\frac{1}{\sqrt{n}} \approx \samplemean{\samplemean{x}} - A_2\samplemean{R} \\
	\text{UCL} &= \samplemean{\samplemean{x}} + 3 \frac{\samplemean{R}}{d_2}\frac{1}{\sqrt{n}} \approx \samplemean{\samplemean{x}} + A_2\samplemean{R}
\end{align*}

\paragraph{Construction of $\samplemean{x}$ and $s$} As there are different ways to measure the variation of a process, a number of different control charts may be employed. One combination that is often used is the $\samplemean{x}$ and $s$ charts. The construction of these two is similar to $\samplemean{x}$ based on $R$, again, the variation is brought under control and then the process standard deviation is estimated using the $\samplemean{x}$ chart.

The centreline of the $s$ chart is denoted $\samplemean{s}$ and is calculated from the standard deviations $s_i$ of the $k$ samples as follows
\begin{equation*}
	\samplemean{s} = \frac{1}{k}\sum_{i=1}^{k} s_i
\end{equation*}
The control limits are then
\begin{align*}
	\text{LCL} &= B_3\samplemean{s}\\
	\text{UCL} &= B_4\samplemean{s}\\
\end{align*}
After the centre line and control limits are determined, the sample standard deviation and lines are plotted in a scatter diagram. Any observation falling outside the control area are omitted and the ranges recalculated.

\paragraph{Construction of $\samplemean{x}$ based on $s$} Using an $s$ chart that is \textbf{under control} the process standard deviation can be estimated by
\begin{equation*}
	\hat{\sigma} = \frac{\samplemean{s}}{c_4}
\end{equation*}
As before, Any samples that were excluded for construction of the $s$ chart should also be disregarded for the construction of the $\samplemean{x}$ chart.

The mean values of the resulting $k^\star$ samples give an estimation of $\mu$
\begin{equation*}
	\samplemean{\samplemean{x}} = \frac{1}{k^\star}\sum_{i=1}^{k^\star}\samplemean{x}_i
\end{equation*}
The control limits are then
\begin{align*}
	\text{LCL} &= \samplemean{\samplemean{x}} - 3 \frac{\samplemean{s}}{c_4}\frac{1}{\sqrt{n}} \approx \samplemean{\samplemean{x}} - A_3\samplemean{s}\\
	\text{UCL} &= \samplemean{\samplemean{x}} + 3 \frac{\samplemean{s}}{c_4}\frac{1}{\sqrt{n}} \approx \samplemean{\samplemean{x}} + A_3\samplemean{s}
\end{align*}

\subsubsection{Individuals Control Charts}
Often its not practical to pick several production pieces for a sample data set, for example if a process is very slow (for example Falcon Heavy Boosters) or if the difference of a repeated measurement cannot be attributed to the process variation, but to the measuring methods. In such situations, it is reasonable to construct a control chart for individual measurements.

The problem that arises is that the variability of the process cannot be estimated from a single measurement. The solution is to use a moving range
\begin{equation*}
	MR_i = \abs{x_{i+1} + x_i}\quad \forall i\in1..n-1
\end{equation*}
The process standard deviation can be estimated from the arithmetic mean of the moving ranges
\begin{equation*}
	\samplemean{MR} = \frac{1}{n-1}\sum_{i=1}^{n-1}MR_i
\end{equation*}
\begin{equation*}
	\hat{\sigma} = \frac{\samplemean{MR}}{d_2}
\end{equation*}
With $d_2$ dependent on the sample size $n$, which would be two in this case as two neighbouring samples were used to calculate the moving range.
The centreline $samplemean{x}$ is the arithmetic mean of the measured values
\begin{equation*}
	\samplemean{x} = \frac{1}{n}\sum_{i=1}^{n}x_i
\end{equation*}
And the control limits
\begin{align*}
	\text{LCL} &= \samplemean{x} - 3\frac{\samplemean{MR}}{d_2}\\
	\text{UCL} &= \samplemean{x} + 3\frac{\samplemean{MR}}{d_2}
\end{align*}

\subsubsection{$p$ Control Chart}
Often we are only interested if a product is usable or defective, which makes the proportion of defectives to tested a discrete random variable. Thus a control chart for attributes data, the $p$ chart, can be used. This chart is cheaper and easier to use, but needs a bigger sample size $n$ than the Shewhart charts.

\vspace{1em}
\noindent
An estimator of success probability is the relative frequency
\begin{equation*}
	\hat{p} = \frac{D}{n}
\end{equation*}
And the variance then
\begin{equation*}
	\Var{\hat{p}} = \frac{p(1-p)}{n}
\end{equation*}
The centre line and the control limits are again determined from a \textbf{stable trial run} with $k^\star$ valid samples. In the event that all $n_i$ are equal
\begin{align*}
	\samplemean{p} &= \frac{1}{k^\star}\sum_{i=1}^{k^\star}p_i\\
	\text{LCL} &= \samplemean{p} - 3 \sqrt{\frac{\samplemean{p}(1-\samplemean{p})}{n}}\\
	\text{UCL} &= \samplemean{p} + 3 \sqrt{\frac{\samplemean{p}(1-\samplemean{p})}{n}}
\end{align*}
In the event that not all $n_i$ are equal
\begin{align*}
	\samplemean{p} &= \frac{d_1 + d_2 + \dots + d_{k^\star}}{n_1 + n_2 + \dots + n_{k^\star}}\\
	\text{LCL}_i &= \samplemean{p} - 3 \sqrt{\frac{\samplemean{p}(1-\samplemean{p})}{n}}\\
	\text{UCL}_i &= \samplemean{p} + 3 \sqrt{\frac{\samplemean{p}(1-\samplemean{p})}{n}}
\end{align*}
The control limits now depend on the index $i$. It might happen that for small sample sizes $n_i$ the control limits $\text{LCL}_i$ are negative, which obviously makes no sense in the context of a probability, in thus correct $\text{LCL}_i$ to $0$.

\subsection{Statistical Properties of Control Charts}
At any given time $t_i$ ($i\in\{1,\dots,k\}$) extract the last $n_i$ parts produced by the machine, calculate the relevant statistic (arithmetic mean for $\samplemean{x}$ chart, range for $R$ chart), and enter the value of the statistic against the index $i$ as a point. If the point is between the control limits the process is \textbf{under control}, otherwise its \textbf{out of control} and the production should be interrupted and the process checked.

\subsubsection{Western Electric Rules}
\begin{enumerate}
	\item Any single data point falls outside the limit defined by UCL and LCL (beyond the $3\sigma$-limit).
	\item Two out of three consecutive points fall beyond the limit defined by $\frac{2}{3}$ UCL and $\frac{2}{3}$ LCL on the same side of the centreline (beyond the $2\sigma$-limit).
	\item  Four out of five consecutive points fall beyond the limit defined by $\frac{1}{3}$ UCL and $\frac{1}{3}$ LCL on the same side of the centreline (beyond the $1\sigma$-limit).
	\item Nine consecutive points fall on the same side of the centreline (a so called \textbf{run}).
\end{enumerate}

\begin{figure}[htb]
	\begin{subfigure}{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/western_electric_rule1.png}
		\caption{Rule 1: Any point beyond zone A}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/western_electric_rule2.png}
		\caption{Rule 2: Two out of three fall into zone A or beyond}
	\end{subfigure}
	\begin{subfigure}{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/western_electric_rule3.png}
		\caption{Rule 3: 4 out of 5 consecutive points fall zone B or beyond}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/western_electric_rule4.png}
		\caption{Rule 4: 9 consecutive points fall on the same side of the centreline}
	\end{subfigure}
\end{figure}

\subsubsection{Type I and Type II Errors}
When monitoring a process with control charts there are two erroneous decisions one can make. If $\mu_0$ is the target value of a process, there are two hypotheses
\begin{itemize}
	\item[$H_0$:] $\mu_0 = \mu$, that is the process is not disturbed
	\item[$H_1$:] $\mu_0 \neq \mu$, that is the process is disturbed, $\mu_1$ is true
\end{itemize}
There are two errors one can make
\begin{enumerate}
	\item Type I: $H_0$ true, but is rejected (\textbf{false alarm})
	\item Type II: $H_0$ false, but is accepted (\textbf{omitted alarm})
\end{enumerate}

\begin{equation*}
	P(\text{Type I}) = \alpha
\end{equation*}

\begin{equation*}
	P(\text{Type II}) = \beta
\end{equation*}
$\beta$ depends on the true value of $\mu_1$ (which is unknown) and $n$.

\begin{figure}[H]
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{img/type_I_correct.png}
		\caption{Since $z < z_{1-\alpha}$ the null hypothesis is accepted. This is the right decision, which is made with probability $1 - \alpha$.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{img/type_I_false.png}
		\caption{Since $z \geq z_{1-\alpha}$ the null hypothesis is rejected. This is the wrong decision (\textbf{type I error}), which is made with probability $\alpha$.}
	\end{subfigure}
	\caption{Outcomes when $H_0$ is true}
\end{figure}

\begin{figure}[H]
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{img/type_II_false.png}
		\caption{Since $z < z_{1-\alpha}$ the null hypothesis is accepted. This is the wrong decision (\textbf{type II error}), which is made with probability $\beta$.}
	\end{subfigure}
	\hfill
	\begin{subfigure}{0.48\linewidth}
		\includegraphics[width=\linewidth]{img/type_II_correct.png}
		\caption{Since $z \geq z_{1-\alpha}$ the null hypothesis is rejected. This is the correct decision, which is made with probability $1 - \beta$ (\textbf{power} of the test).}
	\end{subfigure}
	\caption{Outcomes when $H_0$ is false and alternative hypothesis $H_1$ (dashed density) is true}
\end{figure}

The power of a hypothesis test is the probability $1-\beta$ that the test correctly rejects the null hypothesis when the alternative hypothesis is true
\begin{equation*}
	\text{\textbf{power}} = P(\text{reject $H_0$}|\text{$H_1$ is true}) = 1 - \beta
\end{equation*}
The goal is to keep type I and type II errors small simultaneously, with the complicating factor that a reduction of $\alpha$ increases $\beta$ and vice versa.

\subsection{Power Function}
The power function is the probability of rejecting the null hypothesis $H_0$ if $H_1$ is true, that is the true distribution centres around $\mu_1$. This probability is clearly dependent on the location of $\mu_1$, but we are not interested in $\mu_1$ alone but in the the relation of $\mu_1$ to the target value $\mu_0$ and the process standard deviation $\sigma$. This gives a new normalised variable
\begin{equation*}
	\delta = \frac{\mu_1 - \mu_0}{\sigma}
\end{equation*}
This variable is a measure for the deviation of the observed, disturbed process from the undisturbed process in units of $\sigma$. In statistical process control the \textbf{power function} is denoted by
\begin{equation*}
	g(\mu_1) = g(\delta\sigma + \mu_0) = \tilde{g}(\delta)
\end{equation*}
and is a measure for the probability of intervening in a process.

\subsubsection{Ideal Power Function}
For an undisturbed process where $\mu = \mu_0$ it follows that
\begin{equation*}
	g(\mu_0) = \tilde{g}(0) = \alpha
\end{equation*}
where $\alpha$ is the error probability. In statistical process control $\alpha$ is $0.0027$. For a disturbed process $g(\mu_1)$ should be as large as possible and thus the probability for an omitted alarm $1-g(\mu_1)$ as small as possible. 

\subsubsection{Operating Characteristic}
The operating characteristic curve is the functional relation between the type II error and the position of $\mu_1$ or $\eta = \frac{\mu_1 - \mu_0}{\sigma}$ respectively
\begin{equation*}
	\text{OC}(\mu_1) = 1 - g(\mu_1)
\end{equation*}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{img/OC_curve}
	\caption{An operating characeristic curve for different sample sizes $n$}
	\label{fig:occurve}
\end{figure}

\subsubsection{Average Run Length}
At every time $t_i$ with $i\in\{1,2,\dots,k\}$ a statistical test based on a random sample is made for the control chart. The assumption is to take $n$ random samples at fixed intervals and independent from the production process. The run length $I_{\text{RL}}$ is the time until an intervention in the production process is signalled. The run length is a discrete random variable, which takes values $k\in\{1,2,3,\dots\}$ with the probability
\begin{equation*}
	p_k = P(I_{\text{RL}} = k)
\end{equation*}
The expected value is called \textbf{average run length} (ARL)
\begin{equation*}
	\text{ARL} = \ev{I_{\text{RL}}} = \sum_{k=1}^{\infty} k\cdot p_k
\end{equation*}
The average run length is the mean time until the first intervention in the current process. It depends on the actual mean $\mu_1$ of the process and the control limits.

This chart would be ideal for an undisturbed process, that is $\mu = \mu_0$, thus no intervention or false alarm occurs
\begin{equation*}
	\text{ARL} = \ev{I_{\text{RL}}} = \infty
\end{equation*}
On the other hand, an immediate indication would be signalled with an ideal chart if the process is disturbed so far that an intervention is more economical than further production
\begin{equation*}
	\text{ARL} = \ev{I_{\text{RL}}} = 1
\end{equation*}

\paragraph{Calculation of the Average Run Length} The random variable $I_{\text{RL}}$ follows a geometric distribution with the probabilities
\begin{equation*}
	p_k = P(I_{\text{RL}} = k) = p\cdot(1-p)^{k-1}
\end{equation*}
where $p$ denotes the intervention probability and an expected value $\ev{I_{\text{RL}}} = \frac{1}{p}$.

The probability $p$ to intervene in the process depends on the actual process mean $\mu_1$ and on the control limits UCL and LCL.

\begin{equation*}
	p = p(\mu_1) = P(\mu\leq \text{LCL}) + P(\text{UCL} \leq \mu)
\end{equation*}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/average_run_length}
\end{figure}


\subsubsection{ARL versus Power Function}
The power function of the test is
\begin{equation*}
	p = p(\mu_1) = g(\mu_1) =\tilde{g}(\delta)\quad\text{with}\quad \delta=\frac{\mu_1-\mu_0}{\sigma}
\end{equation*}
Therefore the average run length is
\begin{equation*}
	\text{ARL}(\delta) = \frac{1}{g(\mu_1)} =\frac{1}{\tilde{g}(\delta)}
\end{equation*}
Instead of the power function, the average run length is also used to assess and compare different control charts.

\subsection{Process Capability}
The choice of the error probability $\alpha=0.0027$ implies that $99.73\%$ of all measured values, meaning almost all, lie within the three $\sigma$ range
\begin{equation*}
	[\mu_0 - 3\sigma, \mu_0 + 3\sigma]
\end{equation*}
However, the \textbf{control limits} $\text{LCL}=\mu_0 - 3\frac{\sigma}{\sqrt{n}}$ and $\text{UCL}=\mu_0 + 3\frac{\sigma}{\sqrt{n}}$ define a narrower zone.

\subsection{Tolerance Limits}
Technical tolerances for quality characteristics are defined in industry in the form of two tolerance limits
\begin{itemize}[nosep]
	\item \textbf{lower specification limit} (LSL), and
	\item \textbf{upper specification limit} (USL)
\end{itemize}
which have to be met in production.

If the measured value $x$ of the random variable $X$ is beyond the tolerance limits, then the part is rejected. And a control chart is only suitable for monitoring if the process produces as few rejects as possible.

The performance of a control chart is measured with so called \textbf{capability process ratios} (PCR). The simplest process capability index is
\begin{equation*}
	C_p = \frac{\text{USL} - \text{LSL}}{6\sigma}
\end{equation*}
The capability process ratio $C_p$ expresses the ratio of the width of the tolerance range to the width of the process range.

\vspace{1em}
\noindent
If the process is under control, that is $\mu_1 = \mu_0$ and $\text{SL} = \mu_0$, then $C_p = 1$ implies a reject rate of $\alpha\cdot100\% = 0.27\%$, $C_p < 1$ implies a reject rate greater than $\alpha\cdot100\% = 0.27\%$ and the process capability is \textbf{not} guaranteed, and $C_p > 1$ implies a reject rate less than $\alpha\cdot100\% = 0.27\%$ and the process capability \textbf{is} guaranteed.

In practice the target value $\mu_0$ and the process standard error $\sigma$ are unknown and estimated from a trial run. This results in an estimate for the process capability ratio, which is very sensitive if the process is not under control or the measured values are not normally distributed.

\subsection{Control Charts with Memory}
The decision to interfere with the manufacturing process with classical Shewhart control charts is based on the result of the current sample, and no consideration of the development of the manufacturing process in the past is made (except with the \textit{Western Electric rules}). The solutions is \textbf{modern control charts} with memory.

Given a data set with mean and standard deviation
\begin{tabularx}{\linewidth}{c | c c c c c c | c | c}
	sample & \multicolumn{6}{c |}{measured values} & mean & standard deviation\\
	\hline
	$1$ & $x_{11}$ & $x_{12}$ & $\dots$ & $x_{1j}$ & $\dots$ & $x_{1{\color{red}n_1}}$ & $\samplemean{x}_1$ & $s_1$ \\
	$2$ & $x_{21}$ & $x_{22}$ & $\dots$ & $x_{2j}$ & $\dots$ & $x_{2{\color{red}n_2}}$ & $\samplemean{x}_2$ & $s_2$ \\
	$\vdots$ & $\vdots$ & $\vdots$ &  & $\vdots$ & & $\vdots$ & $\vdots$ & $\vdots$ \\
	$i$ & $x_{i1}$ & $x_{i2}$ & $\dots$ & $x_{ij}$ & $\dots$ & $x_{i{\color{red}n_i}}$ & $\samplemean{x}_i$ & $s_i$ \\
	$\vdots$ & $\vdots$ & $\vdots$ &  & $\vdots$ & & $\vdots$ & $\vdots$ & $\vdots$ \\
	$k$ & $x_{k1}$ & $x_{k2}$ & $\dots$ & $x_{kj}$ & $\dots$ & $x_{k{\color{red}n_k}}$ & $\samplemean{x}_k$ & $s_k$ \\
\end{tabularx}
\begin{itemize}[nosep]
	\item Sample sizes $\color{red}n_i$ may be different
	\item Data comes from a normal distribution
\end{itemize}

\subsubsection{Memory}
The idea is to introduce a linear combination of the mean values $\samplemean{x}_j$ of samples from the past
\begin{equation*}
	y_i = \alpha_i + \sum_{j=1}^{i}\beta_j\samplemean{x}_j
\end{equation*}
where $\alpha_i$ and the weights $\beta_1,\dots,\beta_i$ can be arbitrary real numbers with the sum $\sum_{j=1}^{i}\beta_j = 1$.

\subsubsection{Cumulative Sum Control Chart (CUSUM)}
The idea being that the CUSUM chart plots the cumulative sums of deviations of measurement values from the target value. Given is the mean $\samplemean{x}_i$ of the $i$-th sample of a process, target value $\mu_0$ and the process standard deviation $\sigma$, which is known or estimated from the data.

The goal is to have a control chart that quickly detects a drift from the target value.

\paragraph{Construction of the Cumulative Sum Control Chart} The construction is a recursive procedure using two statistics $C^+$ and $C-$, which sum up the deviations above and below target value respectively
\begin{align*}
	C_i^+ &= \max\left\{0, C_{i-1}^+ + \samplemean{x}_i - (\mu_0 + K)\right\}\\
	C_i^- &= \max\left\{0, C_{i-1}^- - \samplemean{x}_i + (\mu_0 - K)\right\}
\end{align*}
with the starting values $C_0^+ = C_0^- = 0$. If a shift of $\Delta$ is to be detected, set $K=\frac{\Delta}{2}$. This constant $K$ is called \textbf{reference value}.

$C^+$ and $C^-$ only sum up deviations from the target value, which are greater than the reference value $K$. If the process is under control, that is for all $i$
\begin{equation*}
	\mu_0 - K \leq \ev{\samplemean{x}_i}\leq\mu_0+K,
\end{equation*}
then the expected values of the statistics are both zero.
\begin{equation*}
	\ev{C_i^+} = \ev{C_i^-} = 0
\end{equation*}
In this case the two random variables $C_i^+$ and $C_i^-$ fluctuate around the line at zero.

If the process is out of control beyond a certain index $i_0$
\begin{equation*}
	\ev{\samplemean{x}_i} = \mu \geq \mu_0 + K \forall i\geq i_0
\end{equation*}
then the statistic $C^+$ sums up the deviation from $\mu_0 + K$, that is 
\begin{align*}
	\ev{C_i^+} &\geq \left(\mu - (\mu_0 + k)\right)\cdot (i - i_0)\\
	\ev{C_i^-} &= 0
\end{align*}
From the moment $i_0$ where the disturbance occurs, the points $(i,C_i^+)$ lie on average above a straight line with slope $\mu - (\mu_0 + K)$. The same happens for $C^-$ if the process is not under control in the other direction.

The process is \textbf{out of control} if either $C^+$ or $C^-$ exceed a constant $H$, which is called the \textbf{decision interval}. If the process is out of control, it is stopped and the cause investigated. After the problem has been resolved, the CUSUM is restarted from zero.

\paragraph{Choice of $K$ and $H$} As the CUSUM should still have a good ARL, this choice is important. As a rule of thumb, with $\hat{\sigma}$ being an estimate of the process standard deviation
\begin{align*}
	K &= \frac{\hat{\sigma}}{2} \tag{reference value}\\
	H &= 5\hat{\sigma} \tag{decision interval}
\end{align*}

\subsubsection{Exponentially Weighted Moving Average (EWMA)}
The idea is to still monitor the means but have their weights $\beta_j$ decay exponentially. Given are the mean $\samplemean{x}_i$ of the $i$-th sample of a process, the target value $\mu_0$ and the process standard deviation $\sigma$, which is either known or estimated from the data. The goal is to have a control chart that quickly detects a drift from the target value.

\begin{align*}
	\lambda&: 0\leq\lambda\leq 1 \tag{smoothing paramater}\\
	y_i &= \alpha_i + \sum_{j=1}^{i}\beta_j\samplemean{x}_j\\
	\intertext{\textbf{Weights:}}
	\alpha_i &= (1 - \lambda)^i \cdot \mu_0\\
	\beta_j &= \lambda\cdot(1-\lambda)^{i-j}\quad\text{with}\quad j\in\{1,2,\dots,i\}
	\intertext{\textbf{Statistic:}}
	y_i &= (1-\lambda)^i \mu_0 + \lambda\sum_{j=1}^{i}(1-\lambda)^{i-j} \samplemean{x}_j
\end{align*}

\paragraph{Construction of an EWMA Chart} The EWMA chart is constructed in a recursive fashion.
\begin{align*}
	y_0 &= \mu_0 \tag{Initialisation}\\
	y_i &= (1-\lambda)y_{i-1} + \lambda\cdot\samplemean{x}_i \tag{Recursion}
\end{align*}

The \textbf{smoothing parameter} $\lambda$ determines the influence of the previous sample mean $\samplemean{x}_j$ on the statistic
\begin{equation*}
	y_i = (1-\lambda)^i\cdot\mu_0 + \lambda\cdot\sum_{j=1}^{i}(1-\lambda)^{i-j}\samplemean{x}_j
\end{equation*}
The smaller this smoothing parameter is the more previous sample means are used for the decision. The EMWA has thus a non-constant, unlimited memory. If the process is under control, then $\samplemean{x}_i$ comes from a normal distribution with the expected value $\mu_0$ and the standard deviation $\frac{\sigma}{\sqrt{n}}$, whereby the standard deviation is either known or estimated from data. The statistic $y_i$ is then also normally distributed with
\begin{align*}
	\ev{y_i} &= \mu_0
	\intertext{and}
	\Var{y_i} &= \frac{\lambda}{2-\lambda}\left(1 - (1-\lambda)^{2\cdot i}\right)\frac{\sigma^2}{n}
\end{align*}
This results in the $3\sigma$ control limits
\begin{align*}
	\text{LCL}_i = \mu_0 - 3\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\lambda}{2-\lambda}\left(1 - (1-\lambda)^{2\cdot i}\right)}
	\intertext{and}
	\text{UCL}_i = \mu_0 + 3\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\lambda}{2-\lambda}\left(1 - (1-\lambda)^{2\cdot i}\right)}
\end{align*}
A small $i$ results in \textbf{asymptotic control limits}
\begin{align*}
	\text{LCL}_i = \mu_0 - 3\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\lambda}{2-\lambda}}
	\intertext{and}
	\text{UCL}_i = \mu_0 + 3\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\lambda}{2-\lambda}}
\end{align*}
With the \textbf{estimate of the process standard error} $\hat{\sigma}=\frac{\samplemean{s}}{c_4}$ (the factor $c_4$ can be found in table \ref{tbl:FactorsControlCharts}).

\subsection{Acceptance Sampling}
This is a topic of statistical quality control. Certain products can't all be tested, as they get used up during testing (for example single use syringes or bullets). As a compromise draw random samples and make a decision on the quality of the population based on this information.
\begin{itemize}[leftmargin=*, labelindent=3cm, labelsep=0.5cm]
	\item[Lot:] Bulk of a product produced under \textbf{constant conditions}
	\item[Quality Control:] Accept or reject a lot
\end{itemize}
The tool to do this is an acceptance sampling plan. An important remark to make is, that with acceptance sampling one cannot directly intervene in the production process, but only decide whether to accept a delivery or production batch.
\begin{definition}
	An \textbf{acceptance sampling plan} is a set of instructions to decide on the acceptance or return of a lot.
\end{definition}
There are two kinds of acceptance sampling
\begin{itemize}
	\item sampling plans for attributes (counting)
	\item sampling plans for variables (measuring)
\end{itemize}

\subsubsection{Acceptance Sampling Plans for Attributes}
Given a lot with $N$ units called a population of which $m$ are defective parts with $0\leq m\leq N$. Then the percentage of defective parts is
\begin{equation*}
	p = \frac{m}{N}
\end{equation*}
In reality the number of defective parts $m$ and thus the percentage of defective parts $p$ are unknown. The goal is thus to estimate $p$ and to decide whether to accept or reject the lot with the help of a random sample of sample size $n\leq N$. Let $x$ be the number of defective units in the the sample with size $n$.
The acceptance sampling plan consists of the \textbf{sample size} $n$, the \textbf{acceptance number} $c$ and a \textbf{rule}
\begin{itemize}[nosep]
	\item $x\leq c$, then the lot is accepted
	\item $x>c$ the the lot is rejected
\end{itemize}
This procedure corresponds to a statistical hypothesis test (one sided)
\begin{itemize}
	\item $H_0: p=\frac{c}{n}$
	\item $H_1: p>\frac{c}{n}$
	\item Statistic: $X = \#\text{defective units in sample}\in\{0,\dots,n\}$
\end{itemize}
Reject $H_0$ if $\frac{x}{n}>\frac{c}{n}\Leftrightarrow x>c$. Of course errors can be made here too
\begin{itemize}
	\item Type I Error: $H_0$ is true but gets rejected with probability $\alpha = P(x>c)$ (producer's risk)
	\item Type II Error: $H_1$ is true but $H_0$ is accepted with probability $\alpha = P(x\leq c)$ (consumer's risk)
\end{itemize}

\begin{itemize}
	\item \textbf{Probability of Acceptance}: A limit set by the producer $p_{\alpha}$. The producer does not want to take back lots with $p<p_{\alpha}$
	\item \textbf{Probability of Rejection}: A limit set by the consumer $p_{\beta}$. The consumer wants to reject lots with $p_{\beta}<p$ whenever possible.
	\item There must be a $p_{\alpha} < p_{\beta}$
\end{itemize}
The goal is to minimise producer risk $\alpha$ as well as consumer risk $\beta$, without false decisions.

\subsubsection{Operating Characteristic}
The operating characteristic curve plots the probability of accepting a lot as a function of the percent defective $p$. $OC(p)$ is the probability of accepting a lot given that the true percent defective is $p$, called the acceptance probability with $P(X\leq c)$ for $p\in[0,1]$ (depends on $n$ and $c$). A good acceptance sampling plan ensures that the larger the percent
defective $p$, the smaller the OC.

\vspace{0.5em}
\noindent
\textbf{Properties}
\begin{itemize}
	\item Decreases with $p$
	\item $OC(0) = 1$ and $OC(1) = 0$
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/operating_characteristic_curve}
	\caption{Given the quality agreements between producer and consumer ($p_{\alpha}$ and $p_{\beta}$), find the parameters $n$ and $c$ of the acceptance sampling plan}
	\label{fig:operatingcharacteristiccurve}
\end{figure}

In the extreme case of total control and complete information $n=N$ one gets the ideal OC curve
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/ideal_OC_curve}
\end{figure}
But usually $n<N$ and the ideal OC curve is not achievable, but should be approached as closely as possible. In this case, draw random samples and estimate the percent defective $p^\star$.
\begin{equation*}
	OC(p) = P(X\leq c) = \sum_{k=0}^c P(X=k) = \sum_{k=0}^c \frac{\binom{pN}{k}\cdot\binom{N-pN}{n - k}}{\binom{N}{n}}
\end{equation*}

\subsubsection{Find Parameters of Acceptance Sampling Plan}
The goal is to find the parameters $n$ and $c$ of the acceptance sampling plan, with the restriction that the sample size $n$ and the acceptance number $c$ have to be natural numbers with $0\leq c<n \leq N$. The OC curve depends on the parameters and increases with $c$ and decreases with $n$ given a constant $p$.

The requirements given by the opposing parties are
\begin{itemize}
	\item Producer: $OC(p_\alpha) \geq 1-\alpha$
	\item Consumer: $OC(p_\beta) \leq \beta$
\end{itemize}
This results in two points, the \textbf{producer's risk point} $(p_\alpha, 1-\alpha)$ and \textbf{consumer's risk point} $(p_\beta, \beta)$
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/OC_curve_points.png}
	\caption{Sample operating characteristics curve with risk points}
\end{figure}

\subsubsection{R}
For the operating characteristic the hypergeometric distribution is used as \textit{the hypergeometric distribution is used for sampling without replacement}. The density of this distribution with parameters $m, n$ and $k$ (named $Np$, $N-Np$, and $n$, respectively in the reference below, where $N := m+n$ is also used in other references) is given by 
\begin{equation*}
	p(x) = \frac{\binom{m}{x}\binom{n}{k-x}}{\binom{m+n}{k}}
\end{equation*}

In addition we use the function \mintinline{R}{phyper(q,m,n,k)} the following way
\begin{minted}{R}
func.OC <- function(p,N,n,c) { phyper(q=c, m=p*N, n=(1-p)*N, k=n) }
\end{minted}

\section{Multiple Regression}

\subsection{Simple Linear Regression Model}
This model has a single predictor variable which has an effect on the response that conforms to a straight line. It is the most important and widely used statistical technique and helps in the investigation and modelling of relationships between variables.

\subsubsection{Basic Model}
\begin{equation*}
	y = \beta_0 + \beta_1 x + \varepsilon
\end{equation*}
\begin{itemize}
	\item Predictor variable $x$
	\item Response variable $y$
	\item Random error term $\varepsilon$ with $\ev{\varepsilon} = 0$ and $\Var{\varepsilon} = \sigma^2$
\end{itemize}

In real applications, the regression equation is only an approximation to the true functional relationship between the variables of interest. This functional relationship depends on the origin of its model
\begin{itemize}[nosep]
	\item \textbf{Mechanistic models} are based on first principles, that is some physical, chemical, engineering or scientific theory, where the \textit{functional relationship is known}
	\item \textbf{Empirical models} are often based on data, especially in economics, management, life and biological sciences and social sciences, where the \textit{functional relationship is unknown}
\end{itemize}
Regression models and their equations summarise or describe a data set. Their predictions, forecasts, inter- and extrapolation results always come with a confidence interval. To ensure that a model generalises to another data set from the same domain, the determination of significant variables help by reducing the complexity of a model by omitting unimportant variables.

\subsubsection{Remarks}
In real applications, the regression equation is only an approximation to the true functional relationship between the variables of interest.
\begin{itemize}
	\item The regression equations summarise or describe the data set
	\item Predictions, forecasts, inter- or extrapolation are always done with a confidence interval
	\item Reduction of the complexity of a model by omitting unimportant variables
	\item Searching for optimal production conditions
\end{itemize}

\subsubsection{Data Set and Model}
Given a data set of $n$ points
\begin{equation*}
	(x_1,y_1), (x_2,y_2), \dots, (x_n,y_n)
\end{equation*}
and the model
\begin{equation*}
	y = \beta_0 + \beta_1 x + \varepsilon
\end{equation*}
find the parameters $\beta_0$, $\beta_1$ and $\sigma^2$ such that the model fits the data as well as possible.

\paragraph{Method of Least Squares} Minimise:
\begin{equation*}
	S(\beta_0, \beta_1) = \sum_{i=1}^{n} \left(y - (\beta_0 + \beta_1 x_i)\right)^2
\end{equation*}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{img/least_squares_method}
	\caption{Error between data point $x_i,y_i$ and model $y =\beta_0 + \beta_1 x$}
	\label{fig:leastsquaresmethod}
\end{figure}

With $\hat{\beta}_0$ and $\hat{\beta}_1$ being the least-squares estimators of $\beta_0$ and $\beta_1$ respectively, taking the partial derivatives and solving for $0$ results in intercept and slope
\begin{align*}
	\hat{\beta}_0 &= \frac{\frac{1}{n}\sum_{i=1}^{n} x_i^2 \sum_{i=1}^{n}y_i - \frac{1}{n}\sum_{i=1}^{n} x_i^2 \sum_{i=1}^{n}x_i y_i}{\sum_{i=1}^{n}x_i^2 - \frac{1}{n}\left(\sum_{i=1}^{n}\right)^2}\\
	\hat{\beta}_1 &= \frac{\sum_{i=1}^{n}x_i y_i - \frac{1}{n}\sum_{i=1}^{n}x_i\sum_{i=1}^{n}y_i}{\sum_{i=1}^{n}x_i^2 - \frac{1}{n}\left(\sum_{i=1}^{n}\right)^2}
\end{align*}
The solution can also be written as
\begin{align*}
	\hat{\beta}_0 = \samplemean{y} - \hat{\beta}_1\samplemean{x}\quad\text{with}\quad\hat{\beta}_1=\frac{S_{xy}}{S_{xx}}\\
	\intertext{where}
	S_{xx} = \sum_{i=1}^n(x_i - \samplemean{x})^2\quad\text{and}\quad S_{xy} = \sum_{i=1}^{n}(x_i - \samplemean{x})(y_i - \samplemean{y})
	\intertext{with}
	\samplemean{x} = \frac{1}{n}\sum_{i=1}^{n}x_i\quad\text{and}\quad\samplemean{y}=\frac{1}{n}\sum_{i=1}^{n}y_i
\end{align*}
The residuals are the difference between the observed values and their fitted value
\begin{equation*}
	e_i = y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)
\end{equation*}
The expected mean of $e$ is zero
\begin{equation*}
	\samplemean{e} = \frac{1}{n}\sum_{i=1}^{n}\left(y_i - (\hat{\beta}_0 + \hat{\beta}_1 x_i)\right) = 0
\end{equation*}
The error sum of squares is an unbiased estimator for the variance $\sigma^2$ if it is divided by $n-2 $ degrees of freedom
\begin{equation*}
	\sigma^2 = \frac{1}{n-2} \sum_{i=1}^{n} e_i^2
\end{equation*}
The implicit assumption is that all errors $\varepsilon_i$ are normally distributed with expected value $0$ and variance $\sigma^2$, and the estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are normally distributed with
\begin{align*}
	\hat{\beta}_0 &\sim \N{\beta_0, \sigma^2\left(\frac{1}{n}+\frac{\samplemean{x}^2}{S_{xx}}\right)}\\
	\hat{\beta}_1 &\sim \N{\beta_1, \frac{\sigma^2}{S_{xx}}}
\end{align*}
where $S_{xx} = \sum_{i=1}^{n}(x_i - \samplemean{x})^2$. Sample size $n$, sum of squares $S_{xx}$ and the variance of the errors $\sigma^2$ affect the precision of the estimators.

\subsubsection{Statistical Test on the Slope}
Alternative hypotheses:
\begin{align*}
	H_0 &: \beta_1 = \beta_{1,0}\\
	H_1 &: \beta_1 \neq \beta_{1,0}
\end{align*}
Test statistic:
\begin{equation*}
	T = \frac{\hat{\beta}_1 - \beta_{1,0}}{\se{\hat{\beta}_1}}
\end{equation*}
with the standard error
\begin{equation*}
	\se{\hat{\beta}_1} = \sqrt{\frac{\hat{\sigma}^2}{S_{xx}}}\quad\text{with}\quad S_{xx} = \sum_{i=1}^{n}(x_i - \samplemean{x})^2
\end{equation*}

Under the null hypothesis the test statistic $T$ follows a Student's $t$-distribution with $n-2$ degrees of freedom. The $p$-value is the probability, that under the null hypothesis, obtaining a result equal to or more extreme than what was actually estimated. The $p$-value for the two-sided alternative is
\begin{equation*}
	p = P(\beta_1 \leq \abs{\hat{\beta}_1} | H_0\text{ is true}) + P(\abs{\hat{\beta}_1} \leq \beta_1 | H_0\text{ is true})
\end{equation*}
\begin{center}
	\includegraphics[width=0.8\linewidth]{img/p_value}
\end{center}

\paragraph{Statistical Test Procedure with $p$-Value} Given a significance level $\alpha$ (often 5\%) and a calculated probability value $p$. The null hypothesis $H_0$ is rejected and the alternative believed if the $p$-value is smaller than the significance level $\alpha$ ($p < \alpha$). This indicates that the data differs significantly from the null hypothesis. Alternatively, the null hypothesis is accepted if $p\geq\alpha$, which means that the data agrees with the null hypothesis.

\paragraph{Remarks} The idea of a statistical test is to determine if a model can be true or not. The test is a rule that decides which scenario is more probable. The most plausible value for the parameters is the \textbf{least squares estimator}, and if a certain value is plausible is based on a statistical hypothesis test. However, a multitude of values are overall plausible, these form the \textbf{confidence interval}.

\subsubsection{Confidence Interval on the Slope}
The test statistic $ T = \frac{\hat{\beta}_1 - \beta_{1,0}}{\se{\hat{\beta}_1}}$ is accepted on the significance level $\alpha$ if
\begin{equation*}
	t_{\frac{\alpha}{2}, n-2} \leq T \leq t_{1 - \frac{\alpha}{2}, n-2}
\end{equation*}
This is called the \textbf{region of acceptance}. The \textbf{confidence interval on the slope} is
\begin{equation*}
	\hat{\beta}_1 - t_{\frac{\alpha}{2}, n-2}\cdot\se{\hat{\beta}_1} \leq \beta_1 \leq \hat{\beta}_1 + t_{1-\frac{\alpha}{2}, n-2}\cdot\se{\hat{\beta}_1}
\end{equation*}
The width of this confidence interval is a measure of the overall quality of the regression line.

\subsubsection{Confidence Interval on the Response}
The prediction interval will be wider than the confidence interval on the response as the variance of future observations has to be considered too. During the application of a regression model the response $y$ gets estimated for a particular value of the explanatory variable $x$. The \textbf{estimated response} is
\begin{equation*}
	\hat{y}_0 = \hat{\beta}_0 + \hat{\beta}_1 x_0
\end{equation*}
The confidence interval of the response is related to the null hypothesis $H_0: \hat{y}_0 =\mu_0$. The estimator $\hat{y}_0$ is normally distributed and unbiased with
\begin{equation*}
	\Var{\hat{y}_0} = \sigma^2\left(\frac{1}{n} + \frac{(x_0 - \samplemean{x})}{S_{xx}}\right)
\end{equation*}
Giving the test statistic of the response
\begin{equation*}
	T = \frac{\hat{y}_0 - \mu_0}{\se{\hat{y}_0}}
\end{equation*}
with the \textbf{standard error} being
\begin{equation*}
	\se{y_0} = \hat{\sigma} \cdot \sqrt{\frac{1}{n} + \frac{(x_0 - \samplemean{x})^2}{S_{xx}}}\quad\text{and}\quad S_{xx} = \sum_{i=1}^{n}(x_i - \samplemean{x})^2
\end{equation*}
with estimated residual standard error $\hat{\sigma}$.

This gives the confidence interval on the response at the point $x_0$
\begin{equation*}
	\hat{y}_0 - t_{\frac{\alpha}{2}, n-2}\cdot\se{\hat{y}_0} \leq \hat{\beta}_0 + \hat{\beta}_1 x_0 \leq \hat{y}_0 + t_{1 - \frac{\alpha}{2}, n-2}\cdot\se{\hat{y}_0}
\end{equation*}
with $t_{p,m}$ denoting the critical value of the Student's $t$-distribution to the probability $p$ and with $m$ degrees of freedom. The width of this confidence interval depends on the point $x_0$, and is minimal for $x_0 = \samplemean{x}$ and gets bigger as the distance to the sample mean increases.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/confidence_interval_response_vending_machines}
	\caption{Confidence intervals on the response on the vending machines example}
	\label{fig:confidenceintervalresponsevendingmachines}
\end{figure}

\subsubsection{Prediction Intervals}
Assuming that $x_0$ is the value of the explanatory variable of interest, then $\hat{y}_0 = \beta_0 + \beta_1 x_0$ is the point estimate of the new value of the response $y_0$. The goal is now to find a prediction interval for a future observation, considering the randomness of that observation as well.

\paragraph{Variance} Both random variables $y_0$ and $\hat{y}_0$ are normally distributed, and the expected value of $y_0 - \hat{y}_0$ is thus $0$. The two variables are independent as $y_0$ only depends on the future observation and $\hat{y}_0$ only on the past observations. If two random variables $X$ and $Y$ are uncorrelated then
\begin{equation*}
	\Var{X - Y} = \Var{X} + \Var{Y} + 2\underbrace{\Cov{X-Y}}_0
\end{equation*}
Therefore for the two random variables $y_0$ and $\hat{y}_0$
\begin{align*}
	\Var{y_0 - \hat{y}_0} &= \Var{y_0} + \Var{\hat{y}_0}\\
	&= \sigma^2 + \sigma^2\left(\frac{1}{n} + \frac{(x_0 - \samplemean{x})^2}{S_{xx}}\right)
\end{align*}

\paragraph{Prediction Interval} For these two random variables the prediction interval is calculated as follows
\begin{align*}
	\hat{y}_0 - t_{\frac{\alpha}{2}, n-2}\cdot\se{y_0 - \hat{y}_0} \leq \hat{y}_0 \leq \hat{y}_0 + t_{1-\frac{\alpha}{2},n-2}\cdot\se{y_0-\hat{y}_0}\\
	\intertext{with}
	\se{y_0-\hat{y}_0} =\hat{\sigma}\cdot\sqrt{1+\frac{1}{n}+\frac{(x_0 - \samplemean{x})^2}{S_{xx}}}\\
	\intertext{and}
	S_{xx} = \sum_{i=1}^{n}(x_i-\samplemean{x})^2
\end{align*}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/confidence_interval_response_prediction_interval_vending_machines}
	\caption{Prediction and confidence intervals of the vending machines example}
	\label{fig:confidenceintervalresponsepredictionintervalvendingmachines}
\end{figure}

\begin{minted}[
	breaklines=true,
	fontsize=\footnotesize,
	linenos=true,
	stepnumber=5,
	showtabs=true,
	tabsize=4,
	tab=\dotfill
	]{R}
mod <- lm(Time ~ Volume, data)
data.new <- data.frame(Volume=seq(-10, 40, length=101))
Time.Conf <- predict(mod, newdata=data.new, interval="confidence", level=0.95)
Time.Pred <- predict(mod, newdata=data.new, interval="prediction", level=0.95)
#   scatter diagram: Time versus Volume with confidence intervals on the response and prediction intervals
plot(Time ~ Volume, data, pch=20, xlim=c(0,30), ylim=c(0,80), main="Time ~ Volume, best model, CIs on the response, PIs")
grid()
#   add best model
lines(data.new$Volume, Time.Pred[,"fit"], lty=1)
#   add confidence intervals on the response
lines(data.new$Volume, Time.Conf[,"lwr"], lty=2)
lines(data.new$Volume, Time.Conf[,"upr"], lty=2)
#   add prediction intervals
lines(data.new$Volume, Time.Pred[,"lwr"], lty=3)
lines(data.new$Volume, Time.Pred[,"upr"], lty=3)
#   add legend
legend("topleft", legend=c("best model", "confidence intervals on the response", "prediction intervals"), lty=c(1,2,3), bty="n")
\end{minted}

\subsubsection{Recapitulation of Confidence and Prediciton Interval}
\begin{itemize}
	\item Confidence interval is done on the expected response
	\item Prediction interval is for a new observation
\end{itemize}
The prediction interval will be wider than the confidence interval on the response as the variance of a future observation has to be factored in.

%TODO Check if formatting is still fine
\clearpage

\subsection{Residual Analysis}
\begin{equation*}
	y = \beta_0 + \beta_1 x + \varepsilon
\end{equation*}
Model assumptions
\begin{itemize}
	\item Relationship between response y and regressors x is linear
	\item Error $\varepsilon$ has mean $0$
	\item Error $\varepsilon$ has constant variance $\sigma^2$
	\item Errors are uncorrelated
	\item Errors are normally distributed
\end{itemize}
The problem is that departures from these underlying assumptions are not detectable through the $t$-statistic or the coefficient of determination $R^2$. Thus, the residuals should be investigated, as violated assumptions in the errors should be visible there. The residuals $e_i$ are estimators of the random errors.
\begin{equation*}
	e_i = y_i - \hat{y}_i \quad\forall i\in\{1,\dots,n\}
\end{equation*}
\textbf{Standardised residuals}
\begin{equation*}
	\tilde{r}_{\text{std}, i} = \frac{e_i}{\hat{\sigma} \cdot \sqrt{1-\left(\frac{1}{n} + \frac{(x_i - \samplemean{x})^2}{S_{xx}}\right)}}
\end{equation*}
where $\hat{\sigma}$ is estimated from the data. The goal behind residual analysis is less about justification and more about developing a better model from possible deviations.

\paragraph{Exploratory data analysis}
\begin{itemize}
	\item Improve models based on data
	\item Useful methods for designing appropriate models
	\item Mathematical statements, optimality of statistical methods or statistical significance are less important
\end{itemize}

\subsubsection{Coefficient of Determination $R^2$}
The coefficient of determination quantifies the goodness of the fit of the model
\begin{align*}
	R^2 &= \frac{SS_\text{fit}}{S_{yy}}\\
	\intertext{where}
	SS_\text{fit} &= \sum_{i=1}^{n} (\hat{y}_1 - \samplemean{\hat{y}})^2\\
	&= \sum_{i=1}^{n} \left( (\hat{\beta}_0 + \hat{\beta}_1 x_i) - (\hat{\beta}_0 + \hat{\beta}_1 \samplemean{x}) \right)^2\\
	&= \hat{\beta}_1^2 \cdot \sum_{i=1}^{n} (x_i - \samplemean{x})^2\\
	&= \hat{\beta}_1^2 S_{xx}\\
	S_{yy} &= \sum_{i=1}^{n} (y_i - \samplemean{y})^2
\end{align*}
Therefore, the coefficient of determination is
\begin{equation*}
	R^2 = \hat{\beta}_1^2\cdot\frac{S_{xx}}{S_{yy}}
\end{equation*}
leveraging the fact that $\hat{\beta}_1 = \frac{S_{xy}}{S_xx}$ obtains
\begin{align*}
	R^2 &= \frac{S_{xy}^2}{S_{xx}S_{yy}}\\
	&= \Cor{x,y}^2\\
	&= \frac{\left(\sum_{i=1}^{n} (x_i - \samplemean{x})(y_i - \samplemean{y}) \right)^2}{\sum_{i=1}^{n}(x_i - \samplemean{x})^2 \sum_{i=1}^{n}(y_i - \samplemean{y})^2}
\end{align*}
Thus, $R^2$ is the squared correlation between the explanatory variable $x$ and the response $y$.

In the case of multiple models the coefficient of determination is the squared correlation between the response variable $y$ and the fitted values $\hat{y}$
\begin{equation*}
	R^2 = \Cor{y,\hat{y}}^2 = \frac{S_{y\hat{y}}^2}{S_{yy}S_{\hat{y}\hat{y}}}
\end{equation*}
The interpretation being that this measures the linear relationship between response variable and the fit.

\subsubsection{Diagnostic Tools}
\label{sssec:DiagnosticPlots}
\paragraph{Tukey-Anscombe Plot}
Most powerful and indispensable tool of any residual analysis. It plots the residuals $e_i$ against the fitted values $\hat{y}_i$
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\linewidth]{img/TukeyAnscombe.pdf}
	\caption{Visual representation of the residuals $e_i = y_i - \hat{y}_i$}
\end{figure}
The Tukey-Anscombe plot checks the second model assumption that errors have mean zero, which means that the residuals in any interval of the plot should vary randomly around the horizontal line at zero.
Smooth the residuals using LOESS\footnote{locally estimated scatterplot smoothing} and inspect if the smoothed line varies around the horizontal at zero. Compare with bootstrapped sample if the smoothed residuals lie inside stochastic fluctuations.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/tukey_anscombe_plot}
	\caption{Smoothed residual and bootstrapped residuals}
	\label{fig:tukeyanscombeplot}
\end{figure}


\paragraph{Scale-Location Plot}
Is used to check the third model assumption that errors have equal variance. The square-root of the absolute, standardised residuals $\sqrt{\tilde{r}_{\text{std},i}}$ are plotted against the fitted values of $\hat{y}_i$. If the smoothed curve of the square-root of the absolute, standardised residuals is approximately horizontal, the assumption of equal variance of the errors is not violated.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/scale_location_plot}
	\caption{Scale-Location plot with smoothed residuals and bootstrapped residuals}
	\label{fig:scalelocationplot}
\end{figure}


\paragraph{Normal Q-Q Plot}
The normal q-q plot is used to test the fifth model assumption that errors are normally distributed. The quantiles of the empirical distribution of the standardised residuals $\tilde{r}_{\text{std},i}$ are plotted against the quantiles of the normal distribution. If the points in the q-q plot scatter around a straight line, then the assumption that errors are normally distribution holds.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/q-q_plot}
	\caption{Q-Q plot and one with simulated residuals}
	\label{fig:q-qplot}
\end{figure}

\subsection{Treatment of Model Violations}
Through the use of diagnostic tools \textit{characterise inadequacies} of the model, \textit{investigate their cause} and \textit{eliminate problems}. Frequent violations are
\begin{itemize}
	\item Non-constant variance of random errors
	\item Outliers
	\item Heavy-tailed distributions
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/outliers}
	\caption{Example and effect of an outlier}
	\label{fig:outliers}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/heavy_tailed_distribution}
	\caption{Example and effect of a heavy tailed distribution}
	\label{fig:heavytaileddistribution}
\end{figure}

\subsubsection{Tukey's First Aid Transformations}
\begin{itemize}
	\item \textbf{Logarithmic} for continuous positive variables\\
	\begin{flalign*}
		\hspace*{5cm} & z \longmapsto \log(z) &
	\end{flalign*}
	\item \textbf{Square-root} for continuous and discrete positive variables\\
	\begin{flalign*}
		\hspace*{5cm} & z \longmapsto \sqrt{z} &
	\end{flalign*}
	\item \textbf{Arcsine} for proportions\\
	\begin{flalign*}
		\hspace*{5cm} & z \longmapsto \arcsin(\sqrt{z}) &
	\end{flalign*}
	\item \textbf{Logit} for proportions\\
	\begin{flalign*}
		\hspace*{5cm} & z \longmapsto \log\left(\frac{z+{\color{gray}\varepsilon_1}}{1+{\color{gray}\varepsilon_2}-z}\right) &
	\end{flalign*}
\end{itemize}

\subsubsection{Additive and Multiplicative Error Terms}
\textbf{log-log} Transformation of the explanatory and response variable
\begin{align*}
	\tilde{x} &= \log(x)\\
	\tilde{y} &= \log(y)
\end{align*}
The additive error $\varepsilon$
\begin{align*}
	\tilde{y} = \beta_0 + \beta_1 \tilde{x} + \varepsilon\\
	\log(y) = \beta_0 + \beta_1 \log(x) + \varepsilon
\end{align*}
is transformed to a multiplicative model
\begin{equation*}
	y = e^{\beta_0 + \beta_1 \log(x) + \varepsilon} = e^{\beta_0}\cdot e^{\beta_1\log(x)}\cdot e^{\varepsilon}  =e^{\beta_0}\cdot x^{\beta_1}\cdot e^{\varepsilon}
\end{equation*}
with the multiplicative random error $e^\varepsilon$.

\subsection{Correlation Plots}
The fourth model assumption is that errors are independent. However, with chronologically ordered observations like timeseries, errors are correlated, that is neighbour residuals $e_i$ are more similar than residuals far apart.

To find the correlations in the residuals
\begin{itemize}[nosep,label=-]
	\item plot residuals $e_i$ versus index $i$
	\item plot pairs $(e_i,e_{i+1})$ with $i\in \{1,\dots,n-1\}$
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/correlation_plot}
	\caption{Correlation plot to check the assumption of independent errors}
	\label{fig:correlationplot}
\end{figure}
Interpretation:
\begin{itemize}[nosep]
	\vspace{0.5em}
	\item[] Residuals versus Index
	\item \textbf{Negative correlation}: Strong alternation of sign of residuals.
	\item \textbf{No correlation}: No obvious pattern.
	\item \textbf{Positive correlation}: Residuals show same sign over certain periods.
	\vspace{0.5em}
	\item[] Pairs
	\item \textbf{Negative correlation}: Points in ellipse with negative slope.
	\item \textbf{No correlation}: Points show no pattern.
	\item \textbf{Positive correlation}: Points in ellipse with positive slope.
\end{itemize}
The smaller is the ellipse the bigger the correlation over time.

\subsubsection{Remarks}
In many cases the order in the original data set might be lost and the assumption of independence thus can't be validated any more. The problem with correlated errors are that
\begin{itemize}[label=-,nosep]
	\item reported $p$-values are grossly erroneous
	\item confidence intervals are not reliable at all
\end{itemize}

Solutions:
\begin{itemize}
	\item Use \textbf{generalised least squares} (\texttt{gls}, \texttt{nlme})
	\item Add a factor for seasonal effects in the multiple regression model
\end{itemize}

\subsection{Multiple Linear Regression}
\textbf{Multiple Linear Regression Model}
\begin{equation*}
	y = \beta_0 + \beta_1\cdot x_1 + \beta_2\cdot x_2 + \dots + \beta_m\cdot x_m + \varepsilon
\end{equation*}
To find the least-squares estimators $\beta_0,\beta_1,\dots,\beta_m$, for each measurement $X_i$
\begin{equation*}
	y_i = \beta_0 + \sum_{k=1}^{m}\beta_k x_{ik} + \varepsilon_i
\end{equation*}
with $i\in\{1,2,\dots,n\}$.

This is much easier done with linear algebra
\begin{equation*}
	\bm{y} = \begin{pmatrix}
		y_1\\y_2\\\vdots\\y_n
	\end{pmatrix} \quad\text{and}\quad \bm{X} = \begin{pmatrix}
		1 & x_{11} & x_{12} & \cdots & x_{1m}\\
		1 & x_{21} & x_{22} & \cdots & x_{2m}\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		1 & x_{n1} & x_{n2} & \cdots & x_{nm}
	\end{pmatrix}
\end{equation*}
\begin{equation*}
	\bm{\varepsilon} = \begin{pmatrix}
		\varepsilon_1\\\varepsilon_2\\\vdots\\\varepsilon_n
	\end{pmatrix} \quad\text{and}\quad \bm{\beta} = \begin{pmatrix}
		\beta_0\\\beta_1\\\beta_2\\\vdots\\\beta_m
	\end{pmatrix}
\end{equation*}
and thus the model becomes
\begin{equation*}
	\bm{y} = \bm{X\beta + \varepsilon}
\end{equation*}
The goal of the least squares estimator is the same as in the simple linear regression, which is to minimise the residual sum of squares
\begin{align*}
	S(\bm{\beta}) &= \sum_{i=1}^{n}\varepsilon_i^2\\
	&= \varepsilon^t\varepsilon\\
	&= (\bm{y-X\beta})^t (\bm{y-X\beta})\\
	&= \bm{y}^t\bm{y} - 2\bm{\beta}^t\bm{X}^t\bm{y} + \bm{\beta}^t\bm{X}^t\bm{X\beta}
\end{align*}
The least-squares estimators $\hat{\bm{\beta}}$ must satisfy the stationary condition $\frac{\partial S}{\partial \bm{\beta}} = 0$
\begin{equation*}
	\frac{\partial S}{\partial \bm{\beta}}(\hat{\bm{\beta}}) = - 2\bm{X}^t\bm{y} + 2\bm{X}^t\bm{X\beta}
\end{equation*}
Least-squares normal equations
\begin{align*}
	\bm{X}^t\bm{\beta} &= \bm{X}^t\bm{X}\hat{\bm{\beta}}\\
	\hat{\bm{\beta}} &= (\bm{X}^t\bm{X})^{-1}\bm{X}^t\bm{y}\\
	\intertext{Fitted values}
	\hat{\bm{y}} &= \bm{X}\hat{\bm{\beta}}\\
	&= \bm{X}(\bm{X}^t\bm{X})^{-1}\bm{X}^t\bm{y}\\
	&= \bm{H}\bm{y}
\end{align*}
The hat matrix $\bm{H}$ maps the observed values $\bm{y}$ to the fitted values $\hat{\bm{y}}$. It's called that way as it puts $\bm{y}$ a hat on.

The residuals can be calculated the following way
\begin{align*}
	\bm{e} &= \bm{y} - \hat{\bm{y}}\\
	&= \bm{y} - \bm{H}\bm{y}\\
	&= (\bm{I} - \bm{H}) \bm{y}
\end{align*}
where $\bm{I}$ is the $n\times n$ identity matrix.

\subsubsection{Significance Tests of Regression Coefficients}
The assumption is that the least-squares estimators $\hat{\bm{\beta}}$ are normally distributed.
\begin{equation*}
	\ev{\hat{\bm{\beta}}} = \bm{\beta}\quad\text{and}\quad\Cov{\hat{\bm{\beta}}} = \sigma^2(\bm{X}^t\bm{X})^{-1}
\end{equation*}
The null hypothesis $H_0$ is that
\begin{equation*}
	\beta_k = \beta_{k,0}
\end{equation*}
and the test statistic
\begin{equation*}
	T_k = \frac{\hat{\beta}_k - \beta_{k,0}}{\se{\hat{\beta}_k}}\quad\text{with}\quad\se{\hat{\beta}_k} = \hat{\sigma}\sqrt{\left((\bm{X}^t\bm{X})^{-1}\right)_{kk}}
\end{equation*}
with the residual sum of squares $\hat{\sigma}^2 = \frac{\bm{e}^t\bm{e}}{n-(m+1)}$ and $T_k$ following a Student's $t$-distribution with $n-(m+1)$ degrees if freedom. As a conclusion, if the null hypothesis $H_0$ is not rejected, then the regressor $x_k$ can be omitted from the model.

The $100\cdot(1-\alpha)$ percent confidence interval on $\beta_k$ is
\begin{equation*}
	\hat{\beta}_k - t_{\frac{\alpha}{2},n-(m+1)}\cdot\se{\hat{\beta}_k} \leq \beta_k \leq \hat{\beta}_k + t_{1 - \frac{\alpha}{2},n-(m+1)}\cdot\se{\hat{\beta}_k}
\end{equation*}
where $\se{\hat{\beta}_k} = \hat{\sigma}\sqrt{\left((\bm{X}^t\bm{X})^{-1}\right)_{kk}}$, with the residual sum of squares
\begin{equation*}
	\hat{\sigma}^2 = \frac{\bm{e}^t\bm{e}}{n-(m+1)}
\end{equation*}

\subsubsection{Confidence Interval of the Response}
The estimated value $\hat{y}_0$ is an unbiased, normally distributed estimator
\begin{align*}
	\hat{y}_0 &= \hat{\beta}_0 + \hat{\beta}_1 x_{0,1} + \hat{\beta}_2 x_{0,2} + \dots + \hat{\beta}_m x_{0,m}\\
	&=\bm{x}_0^t \hat{\bm{\beta}}\\
	\Var{\hat{y}_0} &= \sigma^2 \bm{x}_0^t(\bm{X}^t\bm{X})^{-1}\bm{x}_0
\end{align*}
with a confidence interval on the response
\begin{equation*}
	\hat{y}_0 - t_{\frac{\alpha}{2},n-(m+1)}\cdot\se{\hat{y}_0} \leq \bm{x}_0^t\hat{\bm{\beta}} \leq \hat{y}_0 + t_{1 - \frac{\alpha}{2},n-(m+1)}\cdot\se{\hat{y}_0}
\end{equation*}
with the standard error being
\begin{equation*}
	\se{\hat{y}_0} = \hat{\sigma}\sqrt{\bm{x}_0^t(\bm{X}^t\bm{X})^{-1}\bm{x}_0}
\end{equation*}
and the residual sum of squares
\begin{equation*}
	\hat{\sigma}^2 = \frac{\bm{e}^t\bm{e}}{n-(m+1)}
\end{equation*}

\subsubsection{Prediction Interval}
The $100(1-\alpha)$ percent prediction interval at the point$(x_{01},x_{02},\dots,x_{0m})$ is

\begin{align*}
	\hat{y}_0 - t_{\frac{\alpha}{2},n-(m+1)}\cdot\sqrt{\hat{\sigma}^2+\se{\hat{y}_0}}^2 \leq \bm{x}_0^t\hat{\bm{\beta}} \leq \hat{y}_0 + t_{1 - \frac{\alpha}{2},n-(m+1)}\cdot\sqrt{\hat{\sigma}^2+\se{\hat{y}_0}}^2
\end{align*}
with
\begin{equation*}
	\se{\hat{y}_0} = \hat{\sigma}\sqrt{\bm{x}_0^t(\bm{X}^t\bm{X})^{-1}\bm{x}_0}
\end{equation*}

\subsubsection{Coefficient of Determination - Multiple $R$-Squared}
The coefficient of determination, \textbf{multiple $R$-squared}, is a measure of the linear relationship between the response variable and the fit, and is calculate as follows
\begin{equation*}
	R^2 = \Cor{y,\hat{y}}^2 = \frac{\left(\sum_{i=1}^{n}(y_i - \samplemean{y})(\hat{y}_i - \samplemean{\hat{y}})\right)^2}{\sum_{i=1}^n(y_i - \samplemean{y})^2\sum_{i=1}^n(\hat{y}_i - \samplemean{\hat{y}})^2}
\end{equation*}
It is identical to the squared correlation between the response variable $y$ and the fitted values $\hat{y}$.

\subsection{Diversity of Modelling Parameters}

\subsubsection{Polynomial Regression}
When there is a curved relationship, low-order polynomial functions can be used. Example model with polynomial of degree two
\begin{equation*}
	y = \beta_0 + \beta_1 x + \beta_2 x^2 + \varepsilon
\end{equation*}
This is a simple linear regression with a quadratic function but only one explanatory variable $x$. The multiple linear regression model is mathematically the same, define $\texttt{lin} = x$ and $\texttt{quad} = x^2$
\begin{equation*}
	y =\beta_0 + \beta_1\cdot\texttt{lin} + \beta_2\cdot\texttt{quad} + \varepsilon
\end{equation*}
Introducing higher powers results in a polynomial regression.

\subsubsection{Non-linear Functions and Linear Regression}
Transforming the variables to get better fitting results may introduce non-linear relationships.
\begin{align*}
	y = \frac{1}{a+be^{-x}} &\longrightarrow \frac{1}{y} = a + be^{-x}\\
	y = \frac{ax}{b+x} &\longleftrightarrow \frac{1}{z} = \frac{1}{a} + \frac{b}{a}\frac{1}{x}\\
	y = ax^b &\longrightarrow \ln{(y)} = \ln{(a)} + b\ln{(x)}
\end{align*}
All these models are linear models, however this is not always possible. Non-linear models can be fitted with the package \mintinline{R}{nls}.

\subsubsection{Binary Explanatory Variables}
A binary variable is discrete with values 0 or 1. A simple binary model would be
\begin{align*}
	y &= \beta_0 + \beta_1 x_{\text{b}} + \varepsilon
	\intertext{then}
	y &= \beta_0 + \beta_1 + \varepsilon \quad\text{if } x_{\text{b}}=1\\
	y &= \beta_0 + \varepsilon \quad\text{if } x_{\text{b}}=0
\end{align*}
The regression model is equivalent to the model of two independent random samples and the difference of their means. This is the unpaired \textbf{Student's t test}.

\subsubsection{Factor Variables}
A factor variable is a discrete variable with $s$ levels $\{l_1, l_2,\dots,l_s\}$. These can be transformed to binary dummy variables $x_k^{(j)} = (x_{k,1}^{(j)}, x_{k,2}^{(j)}, \dots, x_{k,n}^{(j)})$ with
\begin{equation*}
	x_k^{(j)} \left\{
		\begin{matrix*}[l]
			1 & \text{if $i$th observation $x_{k,i}$ is equal to $j$th level $l_j$}\\
			0 & \text{otherwise}
		\end{matrix*}
	\right.
\end{equation*}
The regression model then becomes
\begin{equation*}
	y = \beta_0 + \beta_1x_1 + \dots + (\beta_{k,2}x_k^{(2)} + \dots + \beta_{k,s}x_k^{(s)}) + \dots + \beta_m x_m + \varepsilon
\end{equation*}
To force the uniqueness of the optimisation problem the first level $\beta_{k,1}$ has to be zero, so that there are only $s-1$ dummy variables for a factor variable with $s$ levels.

\subsubsection{Comparison of Multiple Regression Models with an \textit{F}-Test}
The introduction of factors in models was very beneficial for improving accuracy, but it would be nice to have a test which allows to determine if a certain factor has a significant influence. This can be done by comparing two multiple linear regression models.

The complex model is
\begin{equation*}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_m x_m + \varepsilon
\end{equation*}
with $p=m+1$ coefficients. The small model has $q$ coefficients $\beta_{j_1},\dots,\beta_{j_q}$ put to zero, and thus only $p-q$ coefficients. Then the goodness of fit is tested against the complex model.

\paragraph{\textit{F}-Test to Compare the Two Models} The null hypothesis of the $F$-test is
\begin{align*}
	H_0 &: \beta_{j_1} = \beta_{j_2} = \dots = \beta_{j_q} = 0
	\intertext{And the alternative hypothesis}
	H_1 &: \beta_{j_k} \neq 0\hspace*{0.5em}\text{for at least one}\hspace*{0.5em}k\in\{1,\dots,q\}
	\intertext{with the $F$-Statistic}
	F &= \frac{n-p}{q}\frac{\text{SS}_E^* - \text{SS}_E}{\text{SS}_E}
\end{align*}
$\text{SS}_E$ is the sum of squares of the errors of the complex model, and $\text{SS}_E^*$ the sum of squares of the errors of the small model. Under the null hypothesis the $F$-statistic follows an $F$-distribution with $(q,n  p)$ degrees of freedom (see tables \ref{tbl:Fdistribution}-\ref{tbl:FdistributionEnd}).

\subsubsection{Modelling of Two Straight Lines}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.4\linewidth]{img/two_straigt_lines}
	\caption{Straight regression lines through data with two groups}
	\label{fig:twostraigtlines}
\end{figure}
To model this relationship a multiple regression model with a factor variable which indicates group membership must be used
\begin{equation*}
	g_i = \left\{
	\begin{matrix}
		0 & \text{observation $i$ belongs to the first line}\\
		1 & \text{observation $i$ belongs to the second line}
	\end{matrix}
	\right.
\end{equation*}
Model: $y = \alpha + \beta x + \Delta\alpha\cdot g + \Delta\beta\cdot x g + \varepsilon$, can also be expressed as
\begin{equation*}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \beta_3 x_3 + \varepsilon
\end{equation*}
with $x_1 = x$, $x_2 = g$, $x_3 = x g$, $\beta_0 =\alpha$, $\beta_1 = \beta$, $\beta_2=\Delta\alpha$ and $\beta_3 = \Delta\beta$.

\paragraph{Comparison of the Two Straight Lines} Null and alternative hypotheses for the difference of intercept and slopes
\begin{align*}
	H_{\alpha,0} = 0 &\qquad H_{\beta,0} = 0\\
	H_{\alpha,1} \neq 0 &\qquad H_{\beta,1} \neq 0\\
\end{align*}
Individual test statistics:
\begin{itemize}[nosep]
	\item Equal intercepts use the $t$-statistic of the coefficient $\Delta\alpha$
	\item Equal slopes use the $t$-statistic of the coefficient $\Delta\beta$
\end{itemize}
Instead both hypotheses can be tested simultaneously by using the $F$-statistic to compare the models
\begin{align}
	y &= \alpha + \beta x + \Delta\alpha\cdot g + \Delta\beta\cdot x g + \varepsilon\\
	\intertext{and}
	y &= \alpha + \beta x + \varepsilon
\end{align}

\subsection{Sensitivity and Robustness}
In multiple linear regression too many dimensions are involved and the regression model cannot be properly visualised. However, the diagnostic plots introduced in \ref{sssec:DiagnosticPlots} can still be used.
\begin{enumerate}
	\item Analyse and compare with bootstrap simulations
	\begin{itemize}
		\item Tukey-Anscombe plot
		\item scale-location plot
		\item q-q plot
		\item residuals versus leverage
		\item time dependencies in the residuals (plot residuals against index)
	\end{itemize}
	\item Draw conclusions, analyse biggest discrepancies and remedies
\end{enumerate}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/diagnostic_plots_blasting}
	\caption{Diagnostic plots for the multi-dimensional model of the \texttt{Blasting} dataset}
	\label{fig:diagnosticplotsblasting}
\end{figure}
Even though the Tukey-Anscombe diagram shows deviations in the $\N{0,\sigma^2}$ but in multiple regression it's not longer clear which explanatory variable causes deficits.

Thus, a new diagram is introduced which plots the residuals $e$ of each explanatory variable $x_1,\dots,x_k\dots,x_m$ against $x_k$. In this plot the assumption can again be checked.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{img/diagnostic_plots_residuals_explanatory}
	\caption{Plots of residuals against their explanatory variable}
	\label{fig:diagnosticplotsresidualsexplanatory}
\end{figure}

\subsubsection{Influential Observations}
An outlier can be identified by investigating the influence of said data point. This is done by removing the observation, repeating the analysis and measuring the change.\footnote{\href{https://online.stat.psu.edu/stat501/lesson/11/11.1}{PennState Eberly College of Statistics - Distinction Between Outliers \& High Leverage Observations}}

\vspace{1em}
\noindent
\textbf{Cook's distance measure}
\begin{equation*}
	d_i  = \frac{(\hat{\bm{y}}_{\setminus i} - \hat{\bm{y}})^T(\hat{\bm{y}}_{\setminus i} - \hat{\bm{y}})}{p\hat{\sigma}^2}
\end{equation*}
where $\hat{\bm{y}}=\bm{X}\hat{\bm{\beta}}$ is the $n\times1$ vector of the fitted values, $\hat{\bm{y}}_{\setminus i}=\bm{X}\hat{\bm{\beta}}_{\setminus i}$ the $n\times1$ vector of fitted values without observation $i$, and $p$ the number of estimated parameters.

Fortunately the analysis does not have to be repeated $n$ times
\begin{equation*}
	d_i = \frac{e_i^2}{p\hat{\sigma}^2}\frac{h_{ii}}{(1-h_{ii})^2} = \frac{\tilde{e}_{\text{std}, i}^2}{p}\frac{h_{ii}}{(1-h_{ii})^2}
\end{equation*}
where $e_i$ is the $i$th residual and $\tilde{e}_{\text{std},i}$ the standardised residual. Cook's distance measure is a function of the standardised residuals $\tilde{e}_{\text{std},i}$ and the leverages
\begin{equation*}
	h_{ii} = \bm{H}_{ii} = \left(\bm{X}\left(\bm{X}^T\bm{X}\right)^{-1}\bm{X}^T\right)_{ii}
\end{equation*}
The leverages satisfy
\begin{equation*}
	0\leq h_{ii}\leq 1\qquad\text{and}\qquad \bar{h} = \frac{p}{n}
\end{equation*}

\paragraph{Interpretation of Leverages}
If a response $y_i$ is changed by the amount $\Delta_{y_i}$, then $h_{ii}\Delta_{y_i}$ measures the change in the fitted value $\hat{y}_i$. \textit{A large leverage $h_{ii}$ has a big influence on the fitted values}.

The variance of the $i$th residual is 
\begin{equation*}
	\Var{e_i} = (1-h_{ii})\sigma^2
\end{equation*}
\textit{A large leverage $h_{ii}$ reduces the variance of the $i$th residual, and therefore the $i$th observation is close to regression hyper-plane. Leverage is a measure of how far away the explanatory values of an observation are from those of the other observations.}

\paragraph{Identification of Leverage Points}
A good rule of thumb is that observations are dangerous if
\begin{equation*}
	h_{ii} > 2\frac{p}{n}
\end{equation*}

\paragraph{Huber's Classification of Observations} gives a notion of leverage issues
\vspace*{1em}
\begin{tabularx}{\linewidth}{X X}
	\textbf{Leverage} & \textbf{Classification} \\
	\hline
	$h_{ii} < 0.2$ & harmless \\
	$0.2 < h_{ii} < 0.5$ & potentially problematic \\
	$0.5 < h_{ii}$ & should be avoided \\
\end{tabularx}

\paragraph{Identification using Cook's distance measure} with the rule of thumb
\begin{tabularx}{\linewidth}{c X}
	\textbf{Cook's distance} & \textbf{Classification of $i$th observation} \\
	\hline
	$d_i \leq 1$ & harmless \\
	$1 < d_i$ & problematic
\end{tabularx}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.6\linewidth]{img/cook_distance_contour_plot}
	\caption{Contour plot of Cook's distance measure can be used to quickly identify problematic observations. $d(h, \tilde{e}_{\text{std}}) = \frac{\tilde{e}^2_{\text{std}}}{p}\frac{h}{1-h}$}
	\label{fig:cookdistancecontourplot}
\end{figure}

\subsubsection{Weighted Linear Regression}
Define an $n\times n$ diagonal matrix with weights
\begin{equation*}
	\bm{W} = \begin{pmatrix}
		w_1 & 0 & 0 & \cdots & 0\\
		0 & w_2 & 0 & \cdots & 0\\
		0 & 0 & w_3 & \cdots & 0\\
		\vdots & \vdots & \vdots & \ddots & \vdots\\
		0 & 0 & 0 & \cdots & w_n
	\end{pmatrix}
\end{equation*}
and then minimise the weighted residual sum of squares
\begin{align*}
	S(\bm{\beta}) = \sum_{i=1}^{n}w_i\varepsilon_i^2 &= \bm{\varepsilon}^T\bm{\varepsilon}\\
	&= (\bm{y}-\bm{X\beta})^T\bm{W}(\bm{y}-\bm{X\beta})\\
	&= \bm{y}^T\bm{Wy} - 2\bm{\beta}^T\bm{X}^T\bm{Wy} + \bm{\beta}^T\bm{X}^T\bm{WX\beta}
\end{align*}
The least squares estimators $\hat{\bm{\beta}}$ of $\bm{\beta}$ satisfy the stationary condition
\begin{equation*}
	\frac{\partial S}{\partial\bm{\beta}}(\hat{\bm{\beta}}) = -  2\bm{\beta}^T\bm{X}^T\bm{Wy} + 2\bm{X}^T\bm{WX}\hat{\bm{\beta}} = 0
\end{equation*}
Weighted least-squares normal equations
\begin{align*}
	\bm{X}^T\bm{Wy} &= \bm{X}^T\bm{WX}\hat{\bm{\beta}}\\
	\hat{\bm{\beta}} &= \left(\bm{X}^T\bm{WX}\right)^-1\bm{X}^T\bm{Wy}
\end{align*}
The estimate $\hat{\bm{\beta}}$ is unbiased with covariance
\begin{equation*}
	\Cov{\hat{\bm{\beta}}} = \bm{\sigma}^2\left(\bm{X}^T\bm{WX}\right)
\end{equation*}
That means, confidence intervals for the coefficients are available

\subsubsection{Robust Regression}
The aim is to find parameter estimates as if the outliers did not exist. Informal approach to robustness
\begin{enumerate}[nosep]
	\item examine the data for obvious outliers
	\item delete these outliers
	\item apply the optimal inference procedure for the assumed model to the cleaned data set
\end{enumerate}
However, in practice there are several problems
\begin{itemize}
	\item Even expert statisticians do not always screen the data
	\item Difficult or even impossible to identify outliers, particularly in multivariate or highly structured data like multiple linear regression
	\item Relationships in the data cannot be examined without first fitting a model
	\item Difficult to formalise this process so that it can be automatised
	\item Inference based on a standard procedure to the cleaned data is based on distribution theory which ignores the cleaning process and hence will be inapplicable and possibly misleading
\end{itemize}

\subsubsection{Breakdown Point}
The breakdown point is the minimal proportion of incorrect observations which cause completely unrealistic estimates. The aim is to have a robust estimator with a breakdown point of about $\frac{1}{2}$. The arithmetic mean is an estimator with a breakdown point of $\frac{1}{n}$
\begin{equation}
	\samplemean{x} = \frac{1}{n}(x_1 + \dots + x_n)
\end{equation}
can be made arbitrarily large by changing any of the $n$ observations.

The median is a robust estimator with a breakdown point of $\frac{1}{2}$, it can handle almost half a dataset of incorrect data.

\subsubsection{Least Squares Regression}
Minimise
\begin{equation*}
	S(\bm{\beta}) = \sum_{i=1}^n(y_i - \bm{x}_i^T\bm{\beta})^2 = \sum_{i=1}^{n}\rho(y_i - \bm{x}_i^T\bm{\beta})
\end{equation*}
with $\rho(r) = r^2$ being the loss function
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/loss_function}
\end{figure}

\subsubsection{Robust Regression with the M-Estimator}
Observations with large absolute residuals have big influence. Define a new loss function that reduces the  influence of observations with scaled residuals $\frac{e_i}{\hat{\sigma}} > \{c=1.345\}$
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/robust_m_estimator.png}
\end{figure}
\noindent
Find the regression M-estimator $\hat{\bm{\beta}}_M$ by the implicit equations
\begin{equation*}
	\sum_{i=1}^{n}\psi(\tilde{e}_i)\bm{x}_i = 0\quad\text{and}\quad\tilde{e}=\frac{y_i - \bm{x}_i^T\hat{\bm{\beta}}_M}{\sigma}
\end{equation*}
or expressed with weights
\begin{align*}
	\sum_{i=1}^{n}w_i e_i\bm{x}_i &= 0\\
	w_i &= \frac{\psi(\tilde{e}_i)}{\tilde{e}}\\
	\tilde{e}_i = \frac{y_i -\bm{x}_i^T\hat{\bm{\beta}}_M}{\sigma}
\end{align*}
As the weights depend on the residuals $e_i = \bm{x}_i^T\bm{\beta}$, which in turn depend on the parameter $\bm{\beta}$, there is no explicit solution or formula, only iterative algorithms exist.

\subsubsection{Robust Regression with the MM-Estimator}
Leverage points still have big influence. Thus, further reduce the influence of observations with scaled residuals $\frac{e_i}{\hat{\sigma}} > \{b = 4.685\}$ even more with a redescending $\psi$-function.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/robust_mm_estimator.png}
\end{figure}

\subsection{Variable Selection and Modelling}
There are two approaches to modelling, with \textbf{mechanistic models} where the functional relationships are known and the model is used for parameter, confidence and prediction interval estimation. The others are \textbf{empirical models} where the influence of explanatory variables on the response are unknown, and the models are used to investigate the relationship between response and explanatory variables.

Which  explanatory variables should be used in which form in the regression model? With a regression model the explanatory variables $x_1,\dots,x_m$ in
\begin{equation*}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_m x_m + \varepsilon
\end{equation*}
can be transformed into observations $u_k$ like
\begin{equation*}
	x_k = log(u_k) \quad\text{and}\quad x_k = \frac{1}{u_k}
\end{equation*}
or by combination of several observations $u_k, u_l$ like
\begin{equation*}
	x_j = u_k\cdot u_l\quad\text{or}\quad x_j = \frac{u_k + u_l}{2}
\end{equation*}
The same is true for the response $y$.

\subsubsection{Model Selection Methods - Variable Selection Criteria}
\begin{quote}
	\textquotedblleft Plurality must never be posited without necessity\textquotedblright\\
	\hspace*{2em} -William of Ockham
\end{quote}
The idea for better generalising model is to measure model accuracy and penalise model complexity. This leads to a trade-off between model complexity and accuracy.

\paragraph{Adjusted $R$-Squared}
\begin{equation*}
	R_{\text{adj}}^2 = 1 - \frac{n-1}{n-p}(1-R^2) = R^2 - \frac{p-1}{n-p}(1-R^2)
\end{equation*}
with $R^2$ being the coefficient of determination, $n$ being the number of observations and $p=m+1$ being the number of parameters in the model. $R_{\text{adj}}^2\leq R^2$ and $R_{\text{adj}}^2$ gets smaller with increasing number of parameters $p$.

\paragraph{Akaike Information Criterion}
\begin{equation*}
	\text{AIC} = n\ln\left(\frac{1}{n}\sum_{i=1}^{n}e_i^2\right) + 2\tilde{p} + \text{constant}
\end{equation*}
with $n$ being the number of observations, $e_1,\dots,e_n$ being the $n$ residuals and $\tilde{p}$ the number of model parameter with $\sigma$ included.

The Akaike information criterion is the most generalisable criterion and it is also used in time series analysis.

\paragraph{Mallow's $C_p$ Statistic}
\begin{equation*}
	C_p = \frac{\text{SS}_{E}}{\hat{\sigma}_{p^*}^2} + 2p - n = (n-p)\left(\frac{\hat{\sigma}_{p}^2}{\hat{\sigma}_{p^*}^2}\right) + p
\end{equation*}
with $n$ being the number of operations, $\text{SS}_E$ the sum of squares of the errors, $\hat{\sigma}_{p}^2 = \frac{\text{SS}_E}{n-p}$ the estimator of $\sigma^2$ of the model with $p$ parameters, and $\hat{\sigma}_{p^*}^2$ the estimator of $\sigma^2$ of the full model with $p^*$ parameters.

Mallows $C_p$ statistic minimises the prediction error.

\subsubsection{Model Selection Methods - Variable Selection Methods}
\paragraph{Forward Selection}
\begin{enumerate}
	\item In the first step choose the trivial model with an intercept only
	\begin{equation*}
		y=\beta_0 + \varepsilon
	\end{equation*}
	\mintinline{R}{formula = y ~ 1}
	\item In the following steps, the explanatory variable that leads to the largest improvement of the selected model selection criteria is added in the model.
	\item Iterate until no improvement is possible by adding another explanatory variable.
\end{enumerate}

\paragraph{Backward Elimination}
\begin{enumerate}
	\item In the first step choose the full model with all possible explanatory variables
	\begin{equation*}
		y=\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_m x_m + \varepsilon
	\end{equation*}
	\mintinline{R}{formula = y ~ 1}
	\item In the following steps, the explanatory variable that leads to the largest improvement of the selected model selection criteria is added in the model.
	\item Iterate until no improvement is possible by adding another explanatory variable.
\end{enumerate}

\paragraph{Both Directions}
Combination of forward selection and backward elimination
\begin{enumerate}
	\item In the first step choose any model
	\item In the following steps examine whether eliminating or adding an explanatory variable improves the selected model selection criteria. The action that leads to the largest improvement is carried out.
	\item Iterate until no improvement is possible.
\end{enumerate}

\subsubsection{Collinearity}
High correlation coefficients between explanatory variables are allowed in the estimation procedure, but this makes interpretation of the estimated coefficients difficult.
\begin{definition}
	\textbf{Collinearity} is when an offset $\gamma_0$ and coefficients $\gamma_1,\dots,\gamma_{k-1},\gamma_{k+1},\dots,\gamma_m$ exist, such that
	\begin{equation}
		x_k\approx \gamma_0 + \sum_{j,j\neq k}^{m} \gamma_j x_j
		\label{eq:linear_multiple_regression_model}
	\end{equation}
\end{definition}
 If this relation is exact then the least-squares estimator is not uniquely defined and software implementations may have problems.

\paragraph{Measure for Collinearity} with the linear multiple regression model (eq. \ref{eq:linear_multiple_regression_model}) and a coefficient of determination $R_k^2$ for each $x_1,\dots,x_m$.

Then the \textbf{variance inflation factor} of explanatory variable $x_k$ is
\begin{equation*}
	\text{VIF}_k = \frac{1}{1 - R_k^2}
\end{equation*}
This factor is problematic if $\text{VIF} > 5$ (rule of thumb).

\subsubsection{Modelling Strategies}
Humans do modelling not machines!
\begin{itemize}
	\item Automated variable selection depends on chance.\\
	Consider models that are nearly optimal in terms of the selected criteria.
	\item Optimal model do not necessarily have to meet all model requirements.\\
	Residual analysis
	\item Automated variable selection starts with given set of variables.\\
	Transformations and interactions are ignored.
	\item Automated variable selection may provide models and estimated coefficients that contradict first principles.
	\item The concept of a best model may also depend on the purpose.
\end{itemize}
Important points in modelling
\begin{itemize}
	\item \textbf{Residual Analysis}
	\begin{itemize}
		\item Check the assumptions of independence, constant variance, linearity and normal distribution
		\item Identify influential observations and use robust methods as a remedy
	\end{itemize}
	\item \textbf{Transformations}
	\begin{itemize}
		\item Transform the response as well as the explanatory variables
		\item Add interactions
		\item Expand the model with factor variables
	\end{itemize}
	\item \textbf{Variable Selection}\\
	Do not blindly trust automatic variable selection and use your brain!
\end{itemize}

\subsubsection{Overall Modelling Strategy}
\begin{enumerate}
	\item Understanding the problem\\
	Any models already available?
	\item Collection and Processing of data
	\begin{itemize}
		\item Missing values?
		\item Treatment of missing values?
		\item Meaning of null values in variables
		\item First-Aid Transformations of variables
		\item Keep track of data quality
	\end{itemize}
	\item Model fitting, eventually with a robust method
	\item Residual Analysis\\
	If problems are identified, backtrack until problems are remedied
	\item Variable selection and treatment of collinearity
	\item Checking goodness of fit
	\begin{itemize}
		\item Residual analysis with selected models
		\item Match models with first principles
		\item Validate models with data not used in modelling
	\end{itemize}
\end{enumerate}

\subsubsection{Generalisations of Multiple Linear Regression}
\begin{itemize}
	\item \textbf{Non-linear regression}\\
	More complicated functions can be fitted
	\item \textbf{Nonparametric regression}\\
	Less assumptions on the models are made (smoothing)
	\item \textbf{Generalised linear model}\\
	Response variable is allowed to have error distribution models other than normal (Logistic, Poisson and Gamma-regression)
	\item \textbf{Survival analysis}\\
	Censored data (reliability theory, survival and hazard function)
	\item \textbf{Time series analysis}\\
	The errors can be correlated
	\item \textbf{Principal component} and \textbf{partial least squares regression}\\
	More variables than observations: large $p$, small $n$ (chemometrics)
\end{itemize}

\section{Design of Experiments}
\begin{itemize}
	\item[] \textbf{Experiment}
	\item Well defined situation
	\item Analysed variables systematically varied
	\item Background noise under control
	\item[] \textbf{Survey}
	\item Observations in the context of an existing situation
	\item Analysed variables not directly adjustable
	\item Background noise not at all under control
\end{itemize}

\subsection{Terminology and Concepts}
Response variable depends on many explanatory variables
\begin{itemize}
	\item \textbf{Primary variables} are directly connected with the research topic
	\item \textbf{Secondary variables} are also controllable with a reasonable effort
\end{itemize}
Basic idea of an experiment is, that primary and secondary variables can be varied in a prescribed controlled manner.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{img/cause_and_effect_diagram}
	\caption{The cause-and-effect diagram is a tool for identifying all influental factors}
	\label{fig:causeandeffectdiagram}
\end{figure}

\subsubsection{Obtaining Precise Primary Variable Estimates}
\begin{itemize}
	\item For continuous variables $x$ choose values $x_i$ without any condition; choosing $x_i$ equally spaced gives good estimates for the slope and chances to detect deviation from linearity; variance of slope is minimal if sum of squares $S_{xx}$ is maximal (choose wider range of $x$-values)
	\item For factor variables, record the same number of $y$-values for each level
	\item For several different variables make an effort to choose them as independently as possible
	\begin{itemize}[nosep]
		\item For continuous variables choose an orthogonal design
		\item For factors choose a balanced design
	\end{itemize}
\end{itemize}

\subsubsection{Obtaining Precise Secondary Variable Estimates}
\begin{itemize}
	\item[] \textbf{Block designs}\\
	Record additional variables, that is batches, production lot or origin of product
	\item Precision can be increased (if variable is significant)
	\item Interpretability of the results can be simplified
	\item[] \textbf{Randomisation}\\
	Perform experiments in random order
	\item Chronological order as independent as possible from all recorded and unrecorded variables
	\item Prevent influences due to the system of the experimental conditions
	\item Avoid learning effects or aging of a device
\end{itemize}

\subsubsection{Obtaining Precise Response Variable Estimates}
\begin{itemize}
	\item[] \textbf{Replicates}\\
	Multiple measurements with same experimental conditions
	\item Improved accuracy of the statements
	\item Estimate of the variation of the random errors available
	\item Existence of interactions in analysis of variance can be tested
	\item Replicates are not allowed to be measured consecutively, otherwise they are not independent
	\item Randomise replicates despite the increased experimental effort
\end{itemize}
\textbf{Alternative}: Add more explanatory variables, but beware that many studies fail because of too extensive investigations with too few observations.

\subsubsection{Design of Experiments}
The basic principle of experiment design
\begin{itemize}
	\item The main question defines the response variable and the primary explanatory variables and their relevant value range
	\item Record as many variables which might have an influence on the response as possible
	\item Keep secondary variables constant or use them for forming blocks
	\item All continuous explanatory variables should be orthogonal and the factors balanced
	\item The assignment of experimental conditions to study units should be randomised
	\item Independent replicates allow additional model validation
\end{itemize}

\subsubsection{Experimental Design}
An experimental design is a list of experimental conditions that determine how each factor is to be varied and at which levels.

\paragraph{Completely Random Design}
\begin{itemize}[nosep]
	\item One multi-level primary factor variable.
	\item One or more (but always the same number) measurements are made per level
	\item The design is processed in random order to avoid systematic effects
\end{itemize}

\paragraph{Complete Block Design}
\begin{itemize}[nosep]
	\item A secondary factor influences the target variable in addition to the primary factor
	\item Every level of the primary factor is used at least once in each block
\end{itemize}

\paragraph{Complete Factorial Design}
\begin{itemize}[nosep]
	\item Examine $k$ factors with $L$ levels each
	\item Design with $L^k$ measurements is needed
	\item Often too many measurements
\end{itemize}

\paragraph{$\bm{2^k}$ Factorial Design}
\begin{itemize}[nosep]
	\item In screening experiments, many factors are investigated for their influence on the response variable
	\item Examine only 2 levels (\textit{high} and \textit{low}) per factor
\end{itemize}

\paragraph{$\bm{2^k}$ Fractional Factorial Design}
\begin{itemize}[nosep]
	\item To save even more resources
	\item Only a balanced part of the complete $2^k$ design is realised
\end{itemize}

\subsubsection{Plannin and Conducting a Study}
\begin{enumerate}
	\item \textbf{Scientific question}: Avoid situations without precise questions and hypotheses.
	\item \textbf{Response and explanatory variables}: Find all influential factors, for example with a cause-and-effect diagram.
	\item \textbf{Preliminary experiments}: Experience with the topic and the measurement equipment is important
	\item \textbf{Planning}: It is worth going through the question, for example with simulated data. The evaluation of the data in relation to the main questions is clear
	\item \textbf{Sample size}: The magnitude of the random fluctuations and the size of the expected effects has to be known.\\ Often, the question must be downsized
	\item \textbf{Data}: It is important to keep a journal that documents the process of data acquisition and highlights any peculiarities and unforeseen circumstances that may later explain unexpected data discrepancies
	\item \textbf{Data cleanup}: Checking the plausibility of the data and visualising it saves a lot of effort
	\item \textbf{Evaluation, interpretation}: The methodology for evaluating the data in relation to the main question is clear
	\item \textbf{Report, presentation}: For whom is the report mainly intended?
	\item \textbf{Completion}: It is worthwhile to collect and discuss the experiences made with all participants
\end{enumerate}

\section{Factorial Designs and Analysis of Variance}
\subsection{One Factor Analysis of Variance}
Only one statistical test on the linear model
\begin{equation*}
	y_{ij} = \mu_i + \varepsilon_{ij}\quad i\in\{1,2,\dots,g\}\quad j\in\{1,2,\dots m\}
\end{equation*}
The errors $\varepsilon_{ij}$ are normally and independently distributed with mean zero and variance $\sigma^2$.

\vspace{1em}
\noindent
Null and alternative hypothesis:
\begin{align*}
	H_0 &: \mu_1 = \mu_2 = \dots = \mu_g\\
	H_1 &: \mu_i \neq \mu_j\hspace*{0.5em}\text{for at least one pair $i,j$ with $i\neq j$}
\end{align*}

\vspace{1em}
\noindent
Equivalent linear model
\begin{equation*}
	y_{ij} = \mu + \tau_i + \varepsilon_{ij}\quad i\in\{1,2,\dots,g\}\quad j\in\{1,2,\dots m\}
\end{equation*}

\vspace{1em}
\noindent
Null and alternative hypothesis:
\begin{align*}
	H_0 &: \tau_1 = \tau_2 = \dots = \tau_g\\
	H_1 &: \tau_i \neq 0\hspace*{0.5em}\text{for at least one $i$}
\end{align*}
Where mean $\mu$ is common to all treatments, effect $\tau_i$ associated with the $i$th treatment and a random error component $\varepsilon_{ij}$.

\subsubsection{Analysis of Variance (ANOVA)}
Look at the $g$ group means
\begin{equation*}
	\samplemean{y}_{i.} = \frac{1}{m}\sum_{j=1}^{m}y_{ij}\quad\text{for all groups $i\in\{1,2,\dots,g\}$}
\end{equation*}
and compare the variance between group means
\begin{equation*}
	S_b^2 = \frac{1}{g-1}\sum_{i=1}^{g}m\left(\samplemean{y}_{i.} - \samplemean{y}_{i..}\right)^2
\end{equation*}
with variance of the observation within groups
\begin{equation*}
	S_w^2 = \frac{1}{n-g}\sum_{i=1}^{g}\sum_{j=1}^{m}\left(y_{ij} - \samplemean{y}_{i.}\right)^2
\end{equation*}
$\samplemean{y}_{i..}$ is the overall mean (over all the groups) and $n=m\cdot g$ the number of observations.

\vspace*{1em}
\noindent
\textbf{Test Statistic}:
\begin{align*}
	F &= \frac{\text{variance between groups}}{\text{variance within groups}}\\
	&= \frac{\frac{1}{g-1}\sum_{i=1}^{g}m\left(\samplemean{y}_{i.} - \samplemean{y}_{i..}\right)^2}{\frac{1}{n-g}\sum_{i=1}^{g}\sum_{j=1}^{m}\left(y_{ij} - \samplemean{y}_{i.}\right)^2}
\end{align*}
Under the null hypothesis the $F$-statistic follows an $F$-distribution with $(df_G ,df_E ) = (g - 1,n - g)$ degrees of freedom.
\begingroup
\renewcommand{\arraystretch}{1.8}
\begin{tabularx}{\linewidth}{X | p{0.34\linewidth} p{0.2\linewidth} p{0.16\linewidth}}
	Source of Variation & Sum of Squares & Degrees of Freedom & Mean Squares\\
	\hline
	Treatments & $SS_G = \sum_{i=1}^{g}m\left(\samplemean{y}_{i.} - \samplemean{y}_{i..}\right)^2$ & $df_G = g-1$ & $MS_G = \frac{SS_G}{df_G}$\\
	Error & $SS_E = \sum_{i=1}^{g}\sum_{j=1}^{m}\left(y_{ij} - \samplemean{y}_{i.}\right)^2$ & $df_E = n-g$ & $MS_E = \frac{SS_E}{df_E}$\\
	\hline
	Total & $SS_T = \sum_{i=1}^{g}\sum_{j=1}^{m}\left(y_{ij}-\samplemean{y}_{..}\right)^2$ & $df_T = n-1$ & 
\end{tabularx}
\endgroup
\noindent
It is possible to show
\begin{equation*}
	SS_T = SS_G + SS_E
\end{equation*}
Test statistic of the analysis of variance test
\begin{equation*}
	F = \frac{MS_G}{MS_E}
\end{equation*}

\subsection{Two Factor Analysis of Variance}
Linear model for two factors $A$ and $B$ with $a$ and $b$ levels,
\begin{equation*}
	y_{ij} = \mu_{ij} + \varepsilon_{ij}\quad i\in\{1,2,\dots,a\}\quad j\in\{1,2,\dots b\}
\end{equation*}
Ideal value
\begin{equation*}
	\mu_{ij} = \mu + \alpha_i + \beta_j
\end{equation*}
where $\mu$ is the overall value, effect $\alpha_i$ describes factor $A$ and effect $\beta_j$ describes factor $B$.

\subsubsection{Parameter Estimation}
Find estimates for $\mu,\alpha_1,\alpha_2,\dots,\alpha_a,\beta_1,\beta_2,\dots,\beta_b$ with the constraints
\begin{equation*}
	\sum_{i=1}^{a}\alpha_i = 0\quad\text{and}\quad\sum_{j=1}^{b}\beta_j = 0
\end{equation*}
Model
\begin{equation*}
	\mu_{ij} = \mu + \alpha_i + \beta_j + \varepsilon_{ij}
\end{equation*}
with the means
\begin{align*}
	\samplemean{y}_{i.} &= \frac{1}{b}\sum_{j=1}^{b}\left(\mu + \alpha_i + \beta_j + \varepsilon_{ij}\right) = \mu+\alpha_i\\
	\samplemean{y}_{.j} &= \frac{1}{a}\sum_{i=1}^{a}\left(\mu + \alpha_i + \beta_j + \varepsilon_{ij}\right) = \mu+\beta_j\\
	\samplemean{y}_{i.} &= \frac{1}{ab}\sum_{i=1}^{a}\sum_{j=1}^{b}\left(\mu + \alpha_i + \beta_j + \varepsilon_{ij}\right) = \mu
\end{align*}
result in the estimated parameters
\begin{align*}
	\hat{\mu} &= \samplemean{y}_{..}\\
	\hat{\alpha}_i &= \samplemean{y}_{i.} -\samplemean{y}_{..}\quad\forall i\in\{1,2,\dots,a\}\\
	\hat{\beta}_j &= \samplemean{y}_{.j} -\samplemean{y}_{..}\quad\forall j\in\{1,2,\dots,b\}
\end{align*}
with the fitted values
\begin{equation*}
	\hat{y}_{ij} = \hat{\mu} + \hat{\alpha}_i + \hat{\beta}_j
\end{equation*}
and the residuals (differences between observed and fitted values)
\begin{equation*}
	e_{ij} = y_{ij} - \hat{y}_ij
\end{equation*}

\subsubsection{Analysis of Variance for Two Factors}
\begingroup
\renewcommand{\arraystretch}{1.8}
\begin{tabularx}{\linewidth}{X | p{0.16\linewidth} p{0.2\linewidth} p{0.15\linewidth} p{0.15\linewidth}}
	Source of Variation & Sum of Squares & Degrees of Freedom & Mean Squares & Test Statistic\\
	\hline
	Factor A & $SS_A$ & $df_A = a-1$ & $MS_A$ & $\frac{MS_A}{MS_E}$\\
	Factor B & $SS_B$ & $df_B = b-1$ & $MS_B$ & $\frac{MS_B}{MS_E}$\\
	Error & $SS_E$ & $df_E = n-a-b+1$ & $MS_E$ & \\
	\hline
	Total & $SS_T$ & $df_T = n-1$ & &
\end{tabularx}
\endgroup
Null and alternative hypothesis for factor A (equal for B)
\begin{align*}
	H_0 &: \alpha_1=\alpha_2=\dots=\alpha_a=0\\
	H_1 &: \alpha_i\neq0\hspace*{0.5em}\text{for at least one $i$}
\end{align*}
Test statistic
\begin{equation*}
	F_A = \frac{MS_A}{MS_E}
\end{equation*}
Under the null hypothesis the statistic follows an $F$-distribution with $(df_A,df_E ) = (a - 1,n - a - b + 1)$ degrees of freedom.

\subsection{Two Factor Analysis of Variance with Interaction}
Model for $c$ observations per combination $(i,j)$ and two factors
\begin{equation*}
	y_{ijk} = \mu + \alpha_i + \beta_j + \varepsilon_{ijk}\quad i\in\{1,2,\dots,a\}\quad j\in\{1,2,\dots b\}\quad K\in\{1,2,\dots c\}
\end{equation*}
If polygonal lines in the interaction plot are not almost parallel, then an extended model with interaction term $\gamma_{ij}$ is needed
\begin{equation*}
	y_{ijk} = \mu + \alpha_i + \beta_j + \gamma_{ij} + \varepsilon_{ijk}
\end{equation*}
The parameter estimate of the model with interaction needs more than one observation per combination $(i,j)$. To estimate the parameters some additional constraints are needed
\begin{equation*}
	\sum_{i=1}^{a}\gamma_{ij} = 0\quad\text{and}\quad\sum_{j=1}^{b}\gamma_{ij} = 0
\end{equation*}
Which gives the following parameter estimates
\begin{align*}
	\hat{\mu} &= \samplemean{y}_{...}\\
	\hat{\alpha}_i &= \samplemean{y}_{i..} - \samplemean{y}_{...}\\
	\hat{\beta}_i &= \samplemean{y}_{.j.} - \samplemean{y}_{...}\\
	\hat{\gamma}_{ij} &= \samplemean{y}_{ij.} - \samplemean{y}_{i..} - \samplemean{y}_{.j.} - \samplemean{y}_{...}
\end{align*}

\subsubsection{Analysis of Variance Table with Interaction}
\begingroup
\renewcommand{\arraystretch}{1.6}
\begin{tabularx}{\linewidth}{X | p{0.16\linewidth} p{0.2\linewidth} p{0.15\linewidth} p{0.15\linewidth}}
	Source of Variation & Sum of Squares & Degrees of Freedom & Mean Squares & Test Statistic\\
	\hline
	Factor A & $SS_A$ & $df_A = a-1$ & $MS_A$ & $\frac{MS_A}{MS_E}$\\
	Factor B & $SS_B$ & $df_B = b-1$ & $MS_B$ & $\frac{MS_B}{MS_E}$\\
	Interaction I & $SS_I$ & $df_I = (a-1)(b-1)$ & $MS_I$ & $\frac{MS_I}{MS_E}$\\
	Error & $SS_E$ & $df_E = ab(c-1)$ & $MS_E$ & \\
	\hline
	Total & $SS_T$ & $df_T = n-1$ & &
\end{tabularx}
\endgroup
Null and alternative hypothesis for interaction term
\begin{align*}
	H_0 &: \gamma_{11} = \gamma_{12} = \gamma_{13} = \dots = \gamma_{ab} =0\\
	H_1 &: \gamma_{ij}\neq0\hspace*{0.5em}\text{for at least one combination $(i,j)$}
\end{align*}
Test statistic
\begin{equation*}
	F_I = \frac{MS_I}{MS_E}
\end{equation*}
Under the null hypothesis the statistic follows an $F$-distribution with $(df_I,df_E ) = \left(ab(c - 1),(a-1)(b-1)\right)$ degrees of freedom.

\subsubsection{ANOVA with Interaction in \texttt{R}}
In \texttt{R} it is easy to define models with interactions. The data has a structure with two factors \texttt{A} and \texttt{B} and a response \texttt{Y}. The model formula with an interaction term is
\begin{align*}
	\texttt{Y} &\sim \texttt{A} + \texttt{B} + \texttt{A*B}\\
	\intertext{or equivalently}
	\texttt{Y} &\sim \texttt{A*B}
\end{align*}
Full ANOVA:
\begin{minted}{R}
mod <- aov(Y~A+B+A*B, data)
summary(mod)
\end{minted}

\subsection{Screening Experiments}
A process is influenced by many factors, but not all of them may be relevant or significant. The idea is thus to experimentally examine the interaction of factors on the response variable by observing the process under different conditions.

\subsubsection{$2^k$ Factorial Design}
The problem is, that the number of experiments rapidly increases to an unrealistically large number as more factors are involved. The solution is to have only a binary level per factor which leads to $2^k$ runs with $k$ factors.
\begin{tabularx}{\linewidth}{c | c c c | c}
	\multicolumn{5}{l}{\textbf{Design Matrix}}\\[0.5em]
	Run & \multicolumn{3}{c|}{Factors} & Letters\\
	& A & B & C & \\
	\hline
	1 & $-1$ & $-1$ & $-1$ & (1) \\
	2 & $+1$ & $-1$ & $-1$ & a   \\
	3 & $-1$ & $+1$ & $-1$ & b   \\
	4 & $+1$ & $+1$ & $-1$ & ab  \\
	5 & $-1$ & $-1$ & $+1$ & c   \\
	6 & $+1$ & $-1$ & $+1$ & ac  \\
	7 & $-1$ & $+1$ & $+1$ & bc  \\
	8 & $+1$ & $+1$ & $+1$ & abc \\
\end{tabularx}
Design matrix in \textbf{Yates standard order}, but runs of the experiment in randomised order.
\paragraph{Construction of Design Matrix in Plus and Minus Coding Scheme}
\begin{itemize}[nosep,label=]
	\item 1. column: Starting with $-1$, create a column of length $2^k$ by alternating $-1$ and $+1$ values.
	\item 2. column: Create a column of alternating blocks of two $-1$ values and two $+1$ values.
	\item 3. column: Create a column of alternating blocks of four $-1$ values and four $+1$ values.
	\item 4. column: Create a column of alternating blocks of eight $-1$ values and eight $+1$ values.
\end{itemize}
Continue in this manner, using block sizes that are successive powers of 2, until all k columns have been formed.
\paragraph{Estimate main effect} The main effect of a factor is the average response value for all test runs at the high level of the factor minus the average response value for runs at the low level of the factor.

The two factor interaction effect is one-half of the difference between the main effects of one factor calculated at the two levels of the other factor.

\paragraph{Half-Normal Plot} Are used to estimate if effects are significant. If effects are zero then estimates are only noise. Estimated effects are realisations of the same noise. Significant effects show up as outliers. Thus, plot the absolute values of effects versus the quantiles of the normal distribution. The quantile of the $i$th ordered observation is
\begin{equation*}
	q_i = \frac{1}{2}+\frac{1}{2}\frac{i-\frac{3}{8}}{e+\frac{1}{4}}
\end{equation*}
with $e$ being the number of estimated effects.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.8\linewidth]{img/significant_relevant_effects_factorial_design}
	\caption{Estimation of significant or relevant effects using Half-Normal plot and Pareto chart}
	\label{fig:significantrelevanteffectsfactorialdesign}
\end{figure}

\subsubsection{$2^k$ Fractional Factorial Design}
In the full factorial design the phenomena, that few major effects and few interaction effects are significant, can be observed. The idea is thus to combine higher order interactions with experimental error, which leads to a smaller model used to analyse the data. Replace some of the higher-order interactions with additional experimental factors.

\paragraph{Construction} Assign the highest order interaction effect $A:B:C$ to a new main effect $D$. 
\begin{tabularx}{\linewidth}{c | c c c | c}
	Run & \multicolumn{4}{c}{Factors} \\
	& A & B & C & D \\
	\hline
	1 & $-1$ & $-1$ & $-1$ & $-1$ \\
	2 & $+1$ & $-1$ & $-1$ & $+1$ \\
	3 & $-1$ & $+1$ & $-1$ & $+1$ \\
	4 & $+1$ & $+1$ & $-1$ & $-1$ \\
	5 & $-1$ & $-1$ & $+1$ & $+1$ \\
	6 & $+1$ & $-1$ & $+1$ & $-1$ \\
	7 & $-1$ & $+1$ & $+1$ & $-1$ \\
	8 & $+1$ & $+1$ & $+1$ & $+1$ \\
\end{tabularx}
This leads to a significant reduction of runs, at the expense of less information (a $2^{4-1}$ design with 8 runs has less information about the 4 factors than a full $2^4$ design with 16 runs). The new factor $A:B:C$ and $D$ have the same column in the design matrix, thus the ability to differentiate between the influence of the factor and their interaction with the response value is lost. The factors $A:B:C$ and $D$ are called aliased.

\paragraph{Construction of Alias Structure}
\begin{enumerate}
	\item First, write the $p$ assignments of additional factors in equation form. These $p$ equations are called the design generators.
	\item Multiply each generator from Step 1 by its left side to put each generator into the form $I = w$, where $w$ is a word composed of several letters representing particular experimental factors (that is $D = ABC$ becomes $I = ABCD$). It is also possible to create words with minus signs, such as $D = ABC$. If this is done, the resulting design will use a different fraction of the runs from the full $2^k$ design.
	\item  Denote $I = w_1, I = w_2,\dots,I = w_p$ the $p$ design generators from Step 2, form all possible products of the words $w_i$ (one at a time, two at a time, three at a time, etc.). Use the fact that squares of factors can be eliminated. There will be a total of $2^p$ words formed. This collection is called the \textbf{defining relation} of the design.
	\item Multiply each word in the defining relation by all $2^{k  1}$ effects based on k factors. Use the fact that squares of factors cancel out to simplify the products.
\end{enumerate}
The result is called the alias structure of the design.

\paragraph{Example of an Alias Structure Design} Find alias structure of the $2^{41}$ design where $D$ is aliased with $ABC$.
\begin{enumerate}
	\item Only $p = 1$ generator $D = ABC$
	\item Multiplying both sides by $D$ gives $D \cdot D = D(ABC)$ or $I = ABCD$
	\item Defining relation $I = ABCD$
	\item Multiplying each of the $2^{4-1} = 15$ effects by the relation $I = ABCD$ yields to the alias structure.
\end{enumerate}
\begin{tabularx}{\linewidth}{l c l l c l l c l}
	Effect & & Alias & Effect & & Alias & Effect & & Alias \\
	\hline
	$A$ & $=$ & $BCD$ & $AB$ & $=$ & $CD$ & $ABC$ & $=$ & $D$ \\
	$B$ & $=$ & $ACD$ & $AC$ & $=$ & $BD$ & $ABD$ & $=$ & $C$ \\
	$C$ & $=$ & $ABD$ & $AD$ & $=$ & $BC$ & $ACD$ & $=$ & $B$ \\
	$D$ & $=$ & $ABC$ & $BC$ & $=$ & $AD$ & $BCD$ & $=$ & $A$ \\
	    &     &       & $BD$ & $=$ & $AC$ & $ABCD$ &$=$ & $I$ \\
	    &     &       & $CD$ & $=$ & $AB$ &
\end{tabularx}
Eliminating the duplicates yields the final alias structure of the design
\begin{tabularx}{\linewidth}{l c l l c l }
	$A$ & $=$ & $BCD$ & $AB$ & $=$ & $CD$ \\
	$B$ & $=$ & $ACD$ & $AC$ & $=$ & $BD$ \\
	$C$ & $=$ & $ABD$ & $AD$ & $=$ & $BC$ \\
	$D$ & $=$ & $ABC$ & $ABCD$ & $=$ & $I$
\end{tabularx}
The alias structure can be summarised as follows:
\begin{itemize}[nosep]
	\item Each main effect is aliased with a three-factor interaction
	\item All two factor interactions are aliased with one another
	\item The single four-factor interaction is aliased with the grand mean of the data
\end{itemize}

\subsection{Response Surface Methodology}
The aim of this method is to optimise response which is influenced by several explanatory variables.
\begin{itemize}
	\item Detect important factors with a $2^k$ (fractional) factorial design
	\item Estimate main effects, determine direction of steepest ascent
	\item Find global optimum in the determined direction with a suitable design and additional experiments
\end{itemize}
This procedure is the response surface exploration based on a sequence of experimental designs.
\begin{equation*}
	E(y) = \eta = f(x_1, x_2)
\end{equation*}

\begin{figure}[tbh]
	\centering
	\includegraphics[width=0.6\linewidth]{img/contour_plot_second_order_response_surface}
	\caption{Contour plot of second-order response surface}
	\label{fig:contourplotsecondorderresponsesurface}
\end{figure}

The idea behind the response surface methodology is to find the optimal settings for the explanatory variables by experiment data. The response is a first- or second-order polynomial function for several variables, with the optimal fit for function $f$ being found analytically. This procedure is repeated until the optimum is reached.
\begin{equation*} 
	y=f(x_1,\dots,x_k) + \varepsilon \tag{fit response function}
\end{equation*}

\subsubsection{First-Order Design}
The initial settings of process variables are often far away from optimum. To tackle this problem, start with a first-order $2^k$ factorial design with additional measurements in the centre if the test conditions. Approximate the date with the linear model
\begin{equation*}
	y = \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \dots + \beta_k x_k
\end{equation*}
of $k$ explanatory variables.

The advantage of this first order design is that the measurement error can be determined without assuming that the first-order linear model approximates the true response sufficiently well, and that the design can be used to detect deviations (curvature) from the first-order linear model.

\subsubsection{Statistical Test for Curvature in a $2^k$ Design}
\begin{itemize}[leftmargin=*,label=]
	\item \textbf{Hypothesis}
	\begin{itemize}
		\item $H_0$: no curvature in data
		\item $H_1$: curvature in the data
	\end{itemize}
	\item \textbf{Test statistic for curvature}
	\begin{equation*}
		t_{\text{curv}} = \frac{\samplemean{y}_c - \samplemean{y}_f}{\sqrt{s_c^2\left(\frac{1}{n_c} + \frac{1}{2^k}\right)}}
	\end{equation*}
	where $s_c^2$ is the variance of $n_c$ repetitions in the centre of the design.
\end{itemize}

Under the null hypothesis the test statistic $t_{\text{curv}}$ follows a $t$-distribution with $n_c-1$ degrees of freedom.

\subsubsection{Method of Steepest Ascent to find Optimum Setting}
Gradient of response function
\begin{align*}
	f(x_1, x_2) &= \beta_0 + \beta_1\cdot x_1 + \beta_2\cdot x_2\\
	\intertext{is}
	\nabla f(x_1, x_2) &= \begin{pmatrix}
		\frac{\partial}{\partial x_1}f(x_1, x_2)\\
		\frac{\partial}{\partial x_2}f(x_1, x_2)
	\end{pmatrix}\\
	&= \begin{pmatrix}
		\beta_1\\
		\beta_2
	\end{pmatrix}
\end{align*}
Estimated gradient points into the direction of the largest change, or maximal increase of the response, in the coded variable domain.

\subsubsection{Second-Order Response Surface with Interactions}
Second-order polynomials as surface function is used if there is curvature in the system or if the settings are close to the optimum making the response plane almost horizontal. Approximate the data with the linear model
\begin{equation*}
	y = \beta_0 + \sum_{i=1}^k\beta_i\cdot x_i + \sum_{i=1}^{k}\beta_{i,i}\cdot x_i^2 + \sum_{j=1}^k\sum_{i<j}^j\beta_{i,j}x_i x_j + \varepsilon
\end{equation*}
of $k$ explanatory variables, with its squares and all second-order interactions.

\subsubsection{Finding Optimum or Stationary Point}
To find the stationary point of the response function, set all $k$ partial derivatives equal to zero
\begin{align*}
	\frac{\partial}{\partial x_1}f(x_1,\dots,x_k) &= 0\\
	&\vdots\\
	\frac{\partial}{\partial x_k}f(x_1,\dots,x_k) &= 0\\
\end{align*}
and solve this system of $k$ linear equations for the $k$ unknowns $x_1,\dots,x_k$.

However, the solution could be a saddle point, which is more likely the more factors $k$ are involved. If the $k\times k$ Hessian matrix is positive definite for that solution $x_1,\dots,x_k$.
\begin{align*}
	\bm{H}_f &= {\begin{bmatrix}{\dfrac {\partial ^{2}f}{\partial x_{1}^{2}}}&{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{1}\,\partial x_{n}}}\\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{2}^{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{2}\,\partial x_{n}}}\\[2.2ex]\vdots &\vdots &\ddots &\vdots \\[2.2ex]{\dfrac {\partial ^{2}f}{\partial x_{n}\,\partial x_{1}}}&{\dfrac {\partial ^{2}f}{\partial x_{n}\,\partial x_{2}}}&\cdots &{\dfrac {\partial ^{2}f}{\partial x_{n}^{2}}}\end{bmatrix}} \tag{Hessian matrix of function $f$}\\
	x^T\bm{M}x &> 0\hspace*{0.5em}\forall x \tag{positive definiteness of matrix $\bm{M}$}
\end{align*}

\clearpage
\appendix
\section{Tables}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c c c | c c c | c c c | c}
		& \multicolumn{3}{c |}{$\samplemean{x}$ chart} & \multicolumn{3}{c |}{$R$ chart} & \multicolumn{3}{c |}{$s$ chart} & \\
		\hline
		$n$ & $A_1$ & $A_2$ & $A_3$ & $D_3$ & $D_4$ & $d_2$ & $B_3$ & $B_4$ & $c_4$ & $n$\\
		\hline
		$2$ & $3.760$ & $1.880$ & $2.659$ & $0.000$ & $3.267$ & $1.128$ & $0.000$ & $3.267$ & $0.7979$ & $2$ \\
		$3$ & $2.394$ & $1.023$ & $1.954$ & $0.000$ & $2.575$ & $1.693$ & $0.000$ & $2.568$ & $0.8862$ & $3$ \\
		$4$ & $1.880$ & $0.729$ & $1.628$ & $0.000$ & $2.282$ & $2.059$ & $0.000$ & $2.266$ & $0.9213$ & $4$ \\
		$5$ & $1.596$ & $0.577$ & $1.427$ & $0.000$ & $2.115$ & $2.326$ & $0.000$ & $2.089$ & $0.9400$ & $5$ \\
		$6$ & $1.410$ & $0.483$ & $1.287$ & $0.000$ & $2.004$ & $2.534$ & $0.030$ & $1.970$ & $0.9515$ & $6$ \\
		$7$ & $1.277$ & $0.419$ & $1.182$ & $0.076$ & $1.924$ & $2.704$ & $0.118$ & $1.882$ & $0.9594$ & $7$ \\
		$8$ & $1.175$ & $0.373$ & $1.099$ & $0.136$ & $1.864$ & $2.847$ & $0.185$ & $1.815$ & $0.9650$ & $8$ \\
		$9$ & $1.094$ & $0.337$ & $1.032$ & $0.184$ & $1.816$ & $2.970$ & $0.239$ & $1.761$ & $0.9693$ & $9$ \\
		$10$ & $1.028$ & $0.308$ & $0.975$ & $0.223$ & $1.777$ & $3.078$ & $0.284$ & $1.716$ & $0.9727$ & $10$ \\
		$11$ & $0.973$ & $0.285$ & $0.927$ & $0.256$ & $1.744$ & $3.173$ & $0.321$ & $1.679$ & $0.9754$ & $11$ \\
		$12$ & $0.925$ & $0.266$ & $0.886$ & $0.284$ & $1.716$ & $3.258$ & $0.354$ & $1.646$ & $0.9776$ & $12$ \\
		$13$ & $0.884$ & $0.249$ & $0.850$ & $0.308$ & $1.692$ & $3.336$ & $0.382$ & $1.618$ & $0.9794$ & $13$ \\
		$14$ & $0.848$ & $0.235$ & $0.817$ & $0.329$ & $1.671$ & $3.407$ & $0.406$ & $1.594$ & $0.9810$ & $14$ \\
		$15$ & $0.816$ & $0.223$ & $0.789$ & $0.348$ & $1.652$ & $3.472$ & $0.428$ & $1.572$ & $0.9823$ & $15$ \\
		$16$ & $0.788$ & $0.212$ & $0.763$ & $0.364$ & $1.636$ & $3.532$ & $0.448$ & $1.552$ & $0.9835$ & $16$ \\
		$17$ & $0.762$ & $0.203$ & $0.739$ & $0.379$ & $1.621$ & $3.588$ & $0.466$ & $1.534$ & $0.9845$ & $17$ \\
		$18$ & $0.738$ & $0.194$ & $0.718$ & $0.392$ & $1.608$ & $3.640$ & $0.482$ & $1.518$ & $0.9854$ & $18$ \\
		$19$ & $0.717$ & $0.187$ & $0.698$ & $0.404$ & $1.596$ & $3.689$ & $0.497$ & $1.503$ & $0.9862$ & $19$ \\
		$20$ & $0.697$ & $0.180$ & $0.680$ & $0.414$ & $1.586$ & $3.735$ & $0.510$ & $1.490$ & $0.9869$ & $20$ \\
		$21$ & $0.679$ & $0.173$ & $0.663$ & $0.425$ & $1.575$ & $3.778$ & $0.523$ & $1.477$ & $0.9876$ & $21$ \\
		$22$ & $0.662$ & $0.167$ & $0.647$ & $0.434$ & $1.566$ & $3.819$ & $0.534$ & $1.466$ & $0.9882$ & $22$ \\
		$23$ & $0.647$ & $0.162$ & $0.633$ & $0.443$ & $1.557$ & $3.858$ & $0.545$ & $1.455$ & $0.9887$ & $23$ \\
		$24$ & $0.632$ & $0.157$ & $0.619$ & $0.452$ & $1.548$ & $3.895$ & $0.555$ & $1.445$ & $0.9892$ & $24$ \\
		$25$ & $0.619$ & $0.153$ & $0.606$ & $0.459$ & $1.541$ & $3.931$ & $0.565$ & $1.435$ & $0.9896$ & $25$
	\end{tabularx}
	\caption{Factors for constructing control charts}
	\label{tbl:FactorsControlCharts}
\end{table}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$\\
		\hline
		$1$ & $0.990$ & $4052$ & $4999$ & $5404$ & $5624$ & $5764$ & $5859$ & $5928$ & $5981$ & $6022$ & $6056$ \\
		& $0.975$ & $647.8$ & $799.5$ & $864.2$ & $899.6$ & $921.8$ & $937.1$ & $948.2$ & $956.6$ & $963.3$ & $968.6$ \\
		& $0.950$ & $161.4$ & $199.5$ & $215.7$ & $224.6$ & $230.2$ & $234.0$ & $236.8$ & $238.9$ & $240.5$ & $241.9$ \\
		& $0.900$ & $39.86$ & $49.50$ & $53.59$ & $55.83$ & $57.24$ & $58.20$ & $58.91$ & $59.44$ & $59.86$ & $60.19$ \\
		$2$ & $0.990$ & $98.50$ & $99.00$ & $99.16$ & $99.25$ & $99.30$ & $99.33$ & $99.36$ & $99.38$ & $99.39$ & $99.40$ \\
		& $0.975$ & $38.51$ & $39.00$ & $39.17$ & $39.25$ & $39.30$ & $39.33$ & $39.36$ & $39.37$ & $39.39$ & $39.40$ \\
		& $0.950$ & $18.51$ & $19.00$ & $19.16$ & $19.25$ & $19.30$ & $19.33$ & $19.35$ & $19.37$ & $19.38$ & $19.40$ \\
		& $0.900$ & $8.526$ & $9.000$ & $9.162$ & $9.243$ & $9.293$ & $9.326$ & $9.349$ & $9.367$ & $9.381$ & $9.392$ \\
		$3$ & $0.990$ & $34.12$ & $30.82$ & $29.46$ & $28.71$ & $28.24$ & $27.91$ & $27.67$ & $27.49$ & $27.34$ & $27.23$ \\
		& $0.975$ & $17.44$ & $16.04$ & $15.44$ & $15.10$ & $14.88$ & $14.73$ & $14.62$ & $14.54$ & $14.47$ & $14.42$ \\
		& $0.950$ & $10.13$ & $9.552$ & $9.277$ & $9.117$ & $9.013$ & $8.941$ & $8.887$ & $8.845$ & $8.812$ & $8.785$ \\
		& $0.900$ & $5.538$ & $5.462$ & $5.391$ & $5.343$ & $5.309$ & $5.285$ & $5.266$ & $5.252$ & $5.240$ & $5.230$ \\
		$4$ & $0.990$ & $21.20$ & $18.00$ & $16.69$ & $15.98$ & $15.52$ & $15.21$ & $14.98$ & $14.80$ & $14.66$ & $14.55$ \\
		& $0.975$ & $12.22$ & $10.65$ & $9.979$ & $9.604$ & $9.364$ & $9.197$ & $9.074$ & $8.980$ & $8.905$ & $8.844$ \\
		& $0.950$ & $7.709$ & $6.944$ & $6.591$ & $6.388$ & $6.256$ & $6.163$ & $6.094$ & $6.041$ & $5.999$ & $5.964$ \\
		& $0.900$ & $4.545$ & $4.325$ & $4.191$ & $4.107$ & $4.051$ & $4.010$ & $3.979$ & $3.955$ & $3.936$ & $3.920$ \\
		$5$ & $0.990$ & $16.26$ & $13.27$ & $12.06$ & $11.39$ & $10.97$ & $10.67$ & $10.46$ & $10.29$ & $10.16$ & $10.05$ \\
		& $0.975$ & $10.01$ & $8.434$ & $7.764$ & $7.388$ & $7.146$ & $6.978$ & $6.853$ & $6.757$ & $6.681$ & $6.619$ \\
		& $0.950$ & $6.608$ & $5.786$ & $5.409$ & $5.192$ & $5.050$ & $4.950$ & $4.876$ & $4.818$ & $4.772$ & $4.735$ \\
		& $0.900$ & $4.060$ & $3.780$ & $3.619$ & $3.520$ & $3.453$ & $3.405$ & $3.368$ & $3.339$ & $3.316$ & $3.297$ \\
		$6$ & $0.990$ & $13.75$ & $10.92$ & $9.780$ & $9.148$ & $8.746$ & $8.466$ & $8.260$ & $8.102$ & $7.976$ & $7.874$ \\
		& $0.975$ & $8.813$ & $7.260$ & $6.599$ & $6.227$ & $5.988$ & $5.820$ & $5.695$ & $5.600$ & $5.523$ & $5.461$ \\
		& $0.950$ & $5.987$ & $5.143$ & $4.757$ & $4.534$ & $4.387$ & $4.284$ & $4.207$ & $4.147$ & $4.099$ & $4.060$ \\
		& $0.900$ & $3.776$ & $3.463$ & $3.289$ & $3.181$ & $3.108$ & $3.055$ & $3.014$ & $2.983$ & $2.958$ & $2.937$ \\
		$7$ & $0.990$ & $12.25$ & $9.547$ & $8.451$ & $7.847$ & $7.460$ & $7.191$ & $6.993$ & $6.840$ & $6.719$ & $6.620$ \\
		& $0.975$ & $8.073$ & $6.542$ & $5.890$ & $5.523$ & $5.285$ & $5.119$ & $4.995$ & $4.899$ & $4.823$ & $4.761$ \\
		& $0.950$ & $5.591$ & $4.737$ & $4.347$ & $4.120$ & $3.972$ & $3.866$ & $3.787$ & $3.726$ & $3.677$ & $3.637$ \\
		& $0.900$ & $3.589$ & $3.257$ & $3.074$ & $2.961$ & $2.883$ & $2.827$ & $2.785$ & $2.752$ & $2.725$ & $2.703$ \\
		$8$ & $0.990$ & $11.26$ & $8.649$ & $7.591$ & $7.006$ & $6.632$ & $6.371$ & $6.178$ & $6.029$ & $5.911$ & $5.814$ \\
		& $0.975$ & $7.571$ & $6.059$ & $5.416$ & $5.053$ & $4.817$ & $4.652$ & $4.529$ & $4.433$ & $4.357$ & $4.295$ \\
		& $0.950$ & $5.318$ & $4.459$ & $4.066$ & $3.838$ & $3.688$ & $3.581$ & $3.500$ & $3.438$ & $3.388$ & $3.347$ \\
		& $0.900$ & $3.458$ & $3.113$ & $2.924$ & $2.806$ & $2.726$ & $2.668$ & $2.624$ & $2.589$ & $2.561$ & $2.538$ \\
		$9$ & $0.990$ & $10.56$ & $8.022$ & $6.992$ & $6.422$ & $6.057$ & $5.802$ & $5.613$ & $5.467$ & $5.351$ & $5.257$ \\
		& $0.975$ & $7.209$ & $5.715$ & $5.078$ & $4.718$ & $4.484$ & $4.320$ & $4.197$ & $4.102$ & $4.026$ & $3.964$ \\
		& $0.950$ & $5.117$ & $4.256$ & $3.863$ & $3.633$ & $3.482$ & $3.374$ & $3.293$ & $3.230$ & $3.179$ & $3.137$ \\
		& $0.900$ & $3.360$ & $3.006$ & $2.813$ & $2.693$ & $2.611$ & $2.551$ & $2.505$ & $2.469$ & $2.440$ & $2.416$ \\
		$10$ & $0.990$ & $10.04$ & $7.559$ & $6.552$ & $5.994$ & $5.636$ & $5.386$ & $5.200$ & $5.057$ & $4.942$ & $4.849$ \\
		& $0.975$ & $6.937$ & $5.456$ & $4.826$ & $4.468$ & $4.236$ & $4.072$ & $3.950$ & $3.855$ & $3.779$ & $3.717$ \\
		& $0.950$ & $4.965$ & $4.103$ & $3.708$ & $3.478$ & $3.326$ & $3.217$ & $3.135$ & $3.072$ & $3.020$ & $2.978$ \\
		& $0.900$ & $3.285$ & $2.924$ & $2.728$ & $2.605$ & $2.522$ & $2.461$ & $2.414$ & $2.377$ & $2.347$ & $2.323$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$1-10$, m:$1-10$}
	\label{tbl:Fdistribution}
\end{table}

\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $11$ & $12$ & $13$ & $14$ & $15$ & $16$ & $17$ & $18$ & $19$ & $20$\\
		\hline
		$1$ & $0.990$ & $6083$ & $6107$ & $6126$ & $6143$ & $6157$ & $6170$ & $6181$ & $6191$ & $6201$ & $6209$ \\
		& $0.975$ & $973.0$ & $976.7$ & $979.8$ & $982.5$ & $984.9$ & $986.9$ & $988.7$ & $990.3$ & $991.8$ & $993.1$ \\
		& $0.950$ & $243.0$ & $243.9$ & $244.7$ & $245.4$ & $245.9$ & $246.5$ & $246.9$ & $247.3$ & $247.7$ & $248.0$ \\
		& $0.900$ & $60.47$ & $60.71$ & $60.90$ & $61.07$ & $61.22$ & $61.35$ & $61.46$ & $61.57$ & $61.66$ & $61.74$ \\
		$2$ & $0.990$ & $99.41$ & $99.42$ & $99.42$ & $99.43$ & $99.43$ & $99.44$ & $99.44$ & $99.44$ & $99.45$ & $99.45$ \\
		& $0.975$ & $39.41$ & $39.41$ & $39.42$ & $39.43$ & $39.43$ & $39.44$ & $39.44$ & $39.44$ & $39.45$ & $39.45$ \\
		& $0.950$ & $19.40$ & $19.41$ & $19.42$ & $19.42$ & $19.43$ & $19.43$ & $19.44$ & $19.44$ & $19.44$ & $19.45$ \\
		& $0.900$ & $9.401$ & $9.408$ & $9.415$ & $9.420$ & $9.425$ & $9.429$ & $9.433$ & $9.436$ & $9.439$ & $9.441$ \\
		$3$ & $0.990$ & $27.13$ & $27.05$ & $26.98$ & $26.92$ & $26.87$ & $26.83$ & $26.79$ & $26.75$ & $26.72$ & $26.69$ \\
		& $0.975$ & $14.37$ & $14.34$ & $14.30$ & $14.28$ & $14.25$ & $14.23$ & $14.21$ & $14.20$ & $14.18$ & $14.17$ \\
		& $0.950$ & $8.763$ & $8.745$ & $8.729$ & $8.715$ & $8.703$ & $8.692$ & $8.683$ & $8.675$ & $8.667$ & $8.660$ \\
		& $0.900$ & $5.222$ & $5.216$ & $5.210$ & $5.205$ & $5.200$ & $5.196$ & $5.193$ & $5.190$ & $5.187$ & $5.184$ \\
		$4$ & $0.990$ & $14.45$ & $14.37$ & $14.31$ & $14.25$ & $14.20$ & $14.15$ & $14.11$ & $14.08$ & $14.05$ & $14.02$ \\
		& $0.975$ & $8.794$ & $8.751$ & $8.715$ & $8.684$ & $8.657$ & $8.633$ & $8.611$ & $8.592$ & $8.575$ & $8.560$ \\
		& $0.950$ & $5.936$ & $5.912$ & $5.891$ & $5.873$ & $5.858$ & $5.844$ & $5.832$ & $5.821$ & $5.811$ & $5.803$ \\
		& $0.900$ & $3.907$ & $3.896$ & $3.886$ & $3.878$ & $3.870$ & $3.864$ & $3.858$ & $3.853$ & $3.848$ & $3.844$ \\
		$5$ & $0.990$ & $9.963$ & $9.888$ & $9.82$ & $9.77$ & $9.72$ & $9.68$ & $9.64$ & $9.61$ & $9.58$ & $9.55$ \\
		& $0.975$ & $6.568$ & $6.525$ & $6.488$ & $6.456$ & $6.428$ & $6.403$ & $6.381$ & $6.362$ & $6.344$ & $6.329$ \\
		& $0.950$ & $4.704$ & $4.678$ & $4.655$ & $4.636$ & $4.619$ & $4.604$ & $4.590$ & $4.579$ & $4.568$ & $4.558$ \\
		& $0.900$ & $3.282$ & $3.268$ & $3.257$ & $3.247$ & $3.238$ & $3.230$ & $3.223$ & $3.217$ & $3.212$ & $3.207$ \\
		$6$ & $0.990$ & $7.790$ & $7.718$ & $7.657$ & $7.605$ & $7.559$ & $7.519$ & $7.483$ & $7.451$ & $7.422$ & $7.396$ \\
		& $0.975$ & $5.410$ & $5.366$ & $5.329$ & $5.297$ & $5.269$ & $5.244$ & $5.222$ & $5.202$ & $5.184$ & $5.168$ \\
		& $0.950$ & $4.027$ & $4.000$ & $3.976$ & $3.956$ & $3.938$ & $3.922$ & $3.908$ & $3.896$ & $3.884$ & $3.874$ \\
		& $0.900$ & $2.920$ & $2.905$ & $2.892$ & $2.881$ & $2.871$ & $2.863$ & $2.855$ & $2.848$ & $2.842$ & $2.836$ \\
		$7$ & $0.990$ & $6.538$ & $6.469$ & $6.410$ & $6.359$ & $6.314$ & $6.275$ & $6.240$ & $6.209$ & $6.181$ & $6.155$ \\
		& $0.975$ & $4.709$ & $4.666$ & $4.628$ & $4.596$ & $4.568$ & $4.543$ & $4.521$ & $4.501$ & $4.483$ & $4.467$ \\
		& $0.950$ & $3.603$ & $3.575$ & $3.550$ & $3.529$ & $3.511$ & $3.494$ & $3.480$ & $3.467$ & $3.455$ & $3.445$ \\
		& $0.900$ & $2.684$ & $2.668$ & $2.654$ & $2.643$ & $2.632$ & $2.623$ & $2.615$ & $2.607$ & $2.601$ & $2.595$ \\
		$8$ & $0.990$ & $5.734$ & $5.667$ & $5.609$ & $5.559$ & $5.515$ & $5.477$ & $5.442$ & $5.412$ & $5.384$ & $5.359$ \\
		& $0.975$ & $4.243$ & $4.200$ & $4.162$ & $4.130$ & $4.101$ & $4.076$ & $4.054$ & $4.034$ & $4.016$ & $3.999$ \\
		& $0.950$ & $3.313$ & $3.284$ & $3.259$ & $3.237$ & $3.218$ & $3.202$ & $3.187$ & $3.173$ & $3.161$ & $3.150$ \\
		& $0.900$ & $2.519$ & $2.502$ & $2.488$ & $2.475$ & $2.464$ & $2.454$ & $2.446$ & $2.438$ & $2.431$ & $2.425$ \\
		$9$ & $0.990$ & $5.178$ & $5.111$ & $5.055$ & $5.005$ & $4.962$ & $4.924$ & $4.890$ & $4.860$ & $4.833$ & $4.808$ \\
		& $0.975$ & $3.912$ & $3.868$ & $3.831$ & $3.798$ & $3.769$ & $3.744$ & $3.722$ & $3.701$ & $3.683$ & $3.667$ \\
		& $0.950$ & $3.102$ & $3.073$ & $3.048$ & $3.025$ & $3.006$ & $2.989$ & $2.974$ & $2.960$ & $2.948$ & $2.936$ \\
		& $0.900$ & $2.396$ & $2.379$ & $2.364$ & $2.351$ & $2.340$ & $2.330$ & $2.320$ & $2.312$ & $2.305$ & $2.298$ \\
		$10$ & $0.990$ & $4.772$ & $4.706$ & $4.650$ & $4.601$ & $4.558$ & $4.520$ & $4.487$ & $4.457$ & $4.430$ & $4.405$ \\
		& $0.975$ & $3.665$ & $3.621$ & $3.583$ & $3.550$ & $3.522$ & $3.496$ & $3.474$ & $3.453$ & $3.435$ & $3.419$ \\
		& $0.950$ & $2.943$ & $2.913$ & $2.887$ & $2.865$ & $2.845$ & $2.828$ & $2.812$ & $2.798$ & $2.785$ & $2.774$ \\
		& $0.900$ & $2.302$ & $2.284$ & $2.269$ & $2.255$ & $2.244$ & $2.233$ & $2.224$ & $2.215$ & $2.208$ & $2.201$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$1-10$, m:$11-20$}
\end{table}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $25$ & $30$ & $40$ & $50$ & $60$ & $80$ & $100$ & $200$ & $500$ & $\infty$\\
		\hline
		$1$ & $0.990$ & $6240$ & $6260$ & $6286$ & $6302$ & $6313$ & $6326$ & $6334$ & $6350$ & $6360$ & $6366$ \\
		& $0.975$ & $998.1$ & $1001$ & $1006$ & $1008$ & $1010$ & $1012$ & $1013$ & $1016$ & $1017$ & $1018$ \\
		& $0.950$ & $249.3$ & $250.1$ & $251.1$ & $251.8$ & $252.2$ & $252.7$ & $253.0$ & $253.7$ & $254.1$ & $254.3$ \\
		& $0.900$ & $62.05$ & $62.26$ & $62.53$ & $62.69$ & $62.79$ & $62.93$ & $63.01$ & $63.17$ & $63.26$ & $63.33$ \\
		$2$ & $0.990$ & $99.46$ & $99.47$ & $99.48$ & $99.48$ & $99.48$ & $99.48$ & $99.49$ & $99.49$ & $99.50$ & $99.50$ \\
		& $0.975$ & $39.46$ & $39.46$ & $39.47$ & $39.48$ & $39.48$ & $39.49$ & $39.49$ & $39.49$ & $39.50$ & $39.50$ \\
		& $0.950$ & $19.46$ & $19.46$ & $19.47$ & $19.48$ & $19.48$ & $19.48$ & $19.49$ & $19.49$ & $19.49$ & $19.50$ \\
		& $0.900$ & $9.451$ & $9.458$ & $9.466$ & $9.471$ & $9.475$ & $9.479$ & $9.481$ & $9.486$ & $9.489$ & $9.491$ \\
		$3$ & $0.990$ & $26.58$ & $26.50$ & $26.41$ & $26.35$ & $26.32$ & $26.27$ & $26.24$ & $26.18$ & $26.15$ & $26.13$ \\
		& $0.975$ & $14.12$ & $14.08$ & $14.04$ & $14.01$ & $13.99$ & $13.97$ & $13.96$ & $13.93$ & $13.91$ & $13.90$ \\
		& $0.950$ & $8.634$ & $8.617$ & $8.594$ & $8.581$ & $8.572$ & $8.561$ & $8.554$ & $8.540$ & $8.532$ & $8.527$ \\
		& $0.900$ & $5.175$ & $5.168$ & $5.160$ & $5.155$ & $5.151$ & $5.147$ & $5.144$ & $5.139$ & $5.136$ & $5.134$ \\
		$4$ & $0.990$ & $13.91$ & $13.84$ & $13.75$ & $13.69$ & $13.65$ & $13.61$ & $13.58$ & $13.52$ & $13.49$ & $13.46$ \\
		& $0.975$ & $8.501$ & $8.461$ & $8.411$ & $8.381$ & $8.360$ & $8.335$ & $8.319$ & $8.288$ & $8.270$ & $8.257$ \\
		& $0.950$ & $5.769$ & $5.746$ & $5.717$ & $5.699$ & $5.688$ & $5.673$ & $5.664$ & $5.646$ & $5.635$ & $5.628$ \\
		& $0.900$ & $3.828$ & $3.817$ & $3.804$ & $3.795$ & $3.790$ & $3.782$ & $3.778$ & $3.769$ & $3.764$ & $3.761$ \\
		$5$ & $0.990$ & $9.449$ & $9.379$ & $9.29$ & $9.24$ & $9.20$ & $9.16$ & $9.13$ & $9.08$ & $9.04$ & $9.02$ \\
		& $0.975$ & $6.268$ & $6.227$ & $6.175$ & $6.144$ & $6.123$ & $6.096$ & $6.080$ & $6.048$ & $6.028$ & $6.015$ \\
		& $0.950$ & $4.521$ & $4.496$ & $4.464$ & $4.444$ & $4.431$ & $4.415$ & $4.405$ & $4.385$ & $4.373$ & $4.365$ \\
		& $0.900$ & $3.187$ & $3.174$ & $3.157$ & $3.147$ & $3.140$ & $3.132$ & $3.126$ & $3.116$ & $3.109$ & $3.105$ \\
		$6$ & $0.990$ & $7.296$ & $7.229$ & $7.143$ & $7.091$ & $7.057$ & $7.013$ & $6.987$ & $6.934$ & $6.901$ & $6.880$ \\
		& $0.975$ & $5.107$ & $5.065$ & $5.012$ & $4.980$ & $4.959$ & $4.932$ & $4.915$ & $4.882$ & $4.862$ & $4.849$ \\
		& $0.950$ & $3.835$ & $3.808$ & $3.774$ & $3.754$ & $3.740$ & $3.722$ & $3.712$ & $3.690$ & $3.678$ & $3.669$ \\
		& $0.900$ & $2.815$ & $2.800$ & $2.781$ & $2.770$ & $2.762$ & $2.752$ & $2.746$ & $2.734$ & $2.727$ & $2.722$ \\
		$7$ & $0.990$ & $6.058$ & $5.992$ & $5.908$ & $5.858$ & $5.824$ & $5.781$ & $5.755$ & $5.702$ & $5.671$ & $5.650$ \\
		& $0.975$ & $4.405$ & $4.362$ & $4.309$ & $4.276$ & $4.254$ & $4.227$ & $4.210$ & $4.176$ & $4.156$ & $4.142$ \\
		& $0.950$ & $3.404$ & $3.376$ & $3.340$ & $3.319$ & $3.304$ & $3.286$ & $3.275$ & $3.252$ & $3.239$ & $3.230$ \\
		& $0.900$ & $2.571$ & $2.555$ & $2.535$ & $2.523$ & $2.514$ & $2.504$ & $2.497$ & $2.484$ & $2.476$ & $2.471$ \\
		$8$ & $0.990$ & $5.263$ & $5.198$ & $5.116$ & $5.065$ & $5.032$ & $4.989$ & $4.963$ & $4.911$ & $4.880$ & $4.859$ \\
		& $0.975$ & $3.937$ & $3.894$ & $3.840$ & $3.807$ & $3.784$ & $3.756$ & $3.739$ & $3.705$ & $3.684$ & $3.670$ \\
		& $0.950$ & $3.108$ & $3.079$ & $3.043$ & $3.020$ & $3.005$ & $2.986$ & $2.975$ & $2.951$ & $2.937$ & $2.928$ \\
		& $0.900$ & $2.400$ & $2.383$ & $2.361$ & $2.348$ & $2.339$ & $2.328$ & $2.321$ & $2.307$ & $2.298$ & $2.293$ \\
		$9$ & $0.990$ & $4.713$ & $4.649$ & $4.567$ & $4.517$ & $4.483$ & $4.441$ & $4.415$ & $4.363$ & $4.332$ & $4.311$ \\
		& $0.975$ & $3.604$ & $3.560$ & $3.505$ & $3.472$ & $3.449$ & $3.421$ & $3.403$ & $3.368$ & $3.347$ & $3.333$ \\
		& $0.950$ & $2.893$ & $2.864$ & $2.826$ & $2.803$ & $2.787$ & $2.768$ & $2.756$ & $2.731$ & $2.717$ & $2.707$ \\
		& $0.900$ & $2.272$ & $2.255$ & $2.232$ & $2.218$ & $2.208$ & $2.196$ & $2.189$ & $2.174$ & $2.165$ & $2.159$ \\
		$10$ & $0.990$ & $4.311$ & $4.247$ & $4.165$ & $4.115$ & $4.082$ & $4.039$ & $4.014$ & $3.962$ & $3.930$ & $3.909$ \\
		& $0.975$ & $3.355$ & $3.311$ & $3.255$ & $3.221$ & $3.198$ & $3.169$ & $3.152$ & $3.116$ & $3.094$ & $3.080$ \\
		& $0.950$ & $2.730$ & $2.700$ & $2.661$ & $2.637$ & $2.621$ & $2.601$ & $2.588$ & $2.563$ & $2.548$ & $2.538$ \\
		& $0.900$ & $2.174$ & $2.155$ & $2.132$ & $2.117$ & $2.107$ & $2.095$ & $2.087$ & $2.071$ & $2.062$ & $2.055$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$1-10$, m:$25-\infty$}
\end{table}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$\\
		\hline
		$11$ & $0.990$ & $9.646$ & $7.206$ & $6.217$ & $5.668$ & $5.316$ & $5.069$ & $4.886$ & $4.744$ & $4.632$ & $4.539$ \\
		& $0.975$ & $6.724$ & $5.256$ & $4.630$ & $4.275$ & $4.044$ & $3.881$ & $3.759$ & $3.664$ & $3.588$ & $3.526$ \\
		& $0.950$ & $4.844$ & $3.982$ & $3.587$ & $3.357$ & $3.204$ & $3.095$ & $3.012$ & $2.948$ & $2.896$ & $2.854$ \\
		& $0.900$ & $3.225$ & $2.860$ & $2.660$ & $2.536$ & $2.451$ & $2.389$ & $2.342$ & $2.304$ & $2.274$ & $2.248$ \\
		$12$ & $0.990$ & $9.330$ & $6.927$ & $5.953$ & $5.412$ & $5.064$ & $4.821$ & $4.640$ & $4.499$ & $4.388$ & $4.296$ \\
		& $0.975$ & $6.554$ & $5.096$ & $4.474$ & $4.121$ & $3.891$ & $3.728$ & $3.607$ & $3.512$ & $3.436$ & $3.374$ \\
		& $0.950$ & $4.747$ & $3.885$ & $3.490$ & $3.259$ & $3.106$ & $2.996$ & $2.913$ & $2.849$ & $2.796$ & $2.753$ \\
		& $0.900$ & $3.177$ & $2.807$ & $2.606$ & $2.480$ & $2.394$ & $2.331$ & $2.283$ & $2.245$ & $2.214$ & $2.188$ \\
		$13$ & $0.990$ & $9.074$ & $6.701$ & $5.739$ & $5.205$ & $4.862$ & $4.620$ & $4.441$ & $4.302$ & $4.191$ & $4.100$ \\
		& $0.975$ & $6.414$ & $4.965$ & $4.347$ & $3.996$ & $3.767$ & $3.604$ & $3.483$ & $3.388$ & $3.312$ & $3.250$ \\
		& $0.950$ & $4.667$ & $3.806$ & $3.411$ & $3.179$ & $3.025$ & $2.915$ & $2.832$ & $2.767$ & $2.714$ & $2.671$ \\
		& $0.900$ & $3.136$ & $2.763$ & $2.560$ & $2.434$ & $2.347$ & $2.283$ & $2.234$ & $2.195$ & $2.164$ & $2.138$ \\
		$14$ & $0.990$ & $8.862$ & $6.515$ & $5.564$ & $5.035$ & $4.695$ & $4.456$ & $4.278$ & $4.140$ & $4.030$ & $3.939$ \\
		& $0.975$ & $6.298$ & $4.857$ & $4.242$ & $3.892$ & $3.663$ & $3.501$ & $3.380$ & $3.285$ & $3.209$ & $3.147$ \\
		& $0.950$ & $4.600$ & $3.739$ & $3.344$ & $3.112$ & $2.958$ & $2.848$ & $2.764$ & $2.699$ & $2.646$ & $2.602$ \\
		& $0.900$ & $3.102$ & $2.726$ & $2.522$ & $2.395$ & $2.307$ & $2.243$ & $2.193$ & $2.154$ & $2.122$ & $2.095$ \\
		$15$ & $0.990$ & $8.683$ & $6.359$ & $5.417$ & $4.893$ & $4.556$ & $4.318$ & $4.142$ & $4.004$ & $3.895$ & $3.805$ \\
		& $0.975$ & $6.200$ & $4.765$ & $4.153$ & $3.804$ & $3.576$ & $3.415$ & $3.293$ & $3.199$ & $3.123$ & $3.060$ \\
		& $0.950$ & $4.543$ & $3.682$ & $3.287$ & $3.056$ & $2.901$ & $2.790$ & $2.707$ & $2.641$ & $2.588$ & $2.544$ \\
		& $0.900$ & $3.073$ & $2.695$ & $2.490$ & $2.361$ & $2.273$ & $2.208$ & $2.158$ & $2.119$ & $2.086$ & $2.059$ \\
		$16$ & $0.990$ & $8.531$ & $6.226$ & $5.292$ & $4.773$ & $4.437$ & $4.202$ & $4.026$ & $3.890$ & $3.780$ & $3.691$ \\
		& $0.975$ & $6.115$ & $4.687$ & $4.077$ & $3.729$ & $3.502$ & $3.341$ & $3.219$ & $3.125$ & $3.049$ & $2.986$ \\
		& $0.950$ & $4.494$ & $3.634$ & $3.239$ & $3.007$ & $2.852$ & $2.741$ & $2.657$ & $2.591$ & $2.538$ & $2.494$ \\
		& $0.900$ & $3.048$ & $2.668$ & $2.462$ & $2.333$ & $2.244$ & $2.178$ & $2.128$ & $2.088$ & $2.055$ & $2.028$ \\
		$17$ & $0.990$ & $8.400$ & $6.112$ & $5.185$ & $4.669$ & $4.336$ & $4.101$ & $3.927$ & $3.791$ & $3.682$ & $3.593$ \\
		& $0.975$ & $6.042$ & $4.619$ & $4.011$ & $3.665$ & $3.438$ & $3.277$ & $3.156$ & $3.061$ & $2.985$ & $2.922$ \\
		& $0.950$ & $4.451$ & $3.592$ & $3.197$ & $2.965$ & $2.810$ & $2.699$ & $2.614$ & $2.548$ & $2.494$ & $2.450$ \\
		& $0.900$ & $3.026$ & $2.645$ & $2.437$ & $2.308$ & $2.218$ & $2.152$ & $2.102$ & $2.061$ & $2.028$ & $2.001$ \\
		$18$ & $0.990$ & $8.285$ & $6.013$ & $5.092$ & $4.579$ & $4.248$ & $4.015$ & $3.841$ & $3.705$ & $3.597$ & $3.508$ \\
		& $0.975$ & $5.978$ & $4.560$ & $3.954$ & $3.608$ & $3.382$ & $3.221$ & $3.100$ & $3.005$ & $2.929$ & $2.866$ \\
		& $0.950$ & $4.414$ & $3.555$ & $3.160$ & $2.928$ & $2.773$ & $2.661$ & $2.577$ & $2.510$ & $2.456$ & $2.412$ \\
		& $0.900$ & $3.007$ & $2.624$ & $2.416$ & $2.286$ & $2.196$ & $2.130$ & $2.079$ & $2.038$ & $2.005$ & $1.977$ \\
		$19$ & $0.990$ & $8.185$ & $5.926$ & $5.010$ & $4.500$ & $4.171$ & $3.939$ & $3.765$ & $3.631$ & $3.523$ & $3.434$ \\
		& $0.975$ & $5.922$ & $4.508$ & $3.903$ & $3.559$ & $3.333$ & $3.172$ & $3.051$ & $2.956$ & $2.880$ & $2.817$ \\
		& $0.950$ & $4.381$ & $3.522$ & $3.127$ & $2.895$ & $2.740$ & $2.628$ & $2.544$ & $2.477$ & $2.423$ & $2.378$ \\
		& $0.900$ & $2.990$ & $2.606$ & $2.397$ & $2.266$ & $2.176$ & $2.109$ & $2.058$ & $2.017$ & $1.984$ & $1.956$ \\
		$20$ & $0.990$ & $8.096$ & $5.849$ & $4.938$ & $4.431$ & $4.103$ & $3.871$ & $3.699$ & $3.564$ & $3.457$ & $3.368$ \\
		& $0.975$ & $5.871$ & $4.461$ & $3.859$ & $3.515$ & $3.289$ & $3.128$ & $3.007$ & $2.913$ & $2.837$ & $2.774$ \\
		& $0.950$ & $4.351$ & $3.493$ & $3.098$ & $2.866$ & $2.711$ & $2.599$ & $2.514$ & $2.447$ & $2.393$ & $2.348$ \\
		& $0.900$ & $2.975$ & $2.589$ & $2.380$ & $2.249$ & $2.158$ & $2.091$ & $2.040$ & $1.999$ & $1.965$ & $1.937$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$11-20$, m:$1-10$}
\end{table}

\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $11$ & $12$ & $13$ & $14$ & $15$ & $16$ & $17$ & $18$ & $19$ & $20$\\
		\hline
		$11$ & $0.990$ & $4.462$ & $4.397$ & $4.342$ & $4.293$ & $4.251$ & $4.213$ & $4.180$ & $4.150$ & $4.123$ & $4.099$ \\
		& $0.975$ & $3.474$ & $3.430$ & $3.392$ & $3.359$ & $3.330$ & $3.304$ & $3.282$ & $3.261$ & $3.243$ & $3.226$ \\
		& $0.950$ & $2.818$ & $2.788$ & $2.761$ & $2.739$ & $2.719$ & $2.701$ & $2.685$ & $2.671$ & $2.658$ & $2.646$ \\
		& $0.900$ & $2.227$ & $2.209$ & $2.193$ & $2.179$ & $2.167$ & $2.156$ & $2.147$ & $2.138$ & $2.130$ & $2.123$ \\
		$12$ & $0.990$ & $4.220$ & $4.155$ & $4.100$ & $4.052$ & $4.010$ & $3.972$ & $3.939$ & $3.910$ & $3.883$ & $3.858$ \\
		& $0.975$ & $3.321$ & $3.277$ & $3.239$ & $3.206$ & $3.177$ & $3.152$ & $3.129$ & $3.108$ & $3.090$ & $3.073$ \\
		& $0.950$ & $2.717$ & $2.687$ & $2.660$ & $2.637$ & $2.617$ & $2.599$ & $2.583$ & $2.568$ & $2.555$ & $2.544$ \\
		& $0.900$ & $2.166$ & $2.147$ & $2.131$ & $2.117$ & $2.105$ & $2.094$ & $2.084$ & $2.075$ & $2.067$ & $2.060$ \\
		$13$ & $0.990$ & $4.025$ & $3.960$ & $3.905$ & $3.857$ & $3.815$ & $3.778$ & $3.745$ & $3.716$ & $3.689$ & $3.665$ \\
		& $0.975$ & $3.197$ & $3.153$ & $3.115$ & $3.082$ & $3.053$ & $3.027$ & $3.004$ & $2.983$ & $2.965$ & $2.948$ \\
		& $0.950$ & $2.635$ & $2.604$ & $2.577$ & $2.554$ & $2.533$ & $2.515$ & $2.499$ & $2.484$ & $2.471$ & $2.459$ \\
		& $0.900$ & $2.116$ & $2.097$ & $2.080$ & $2.066$ & $2.053$ & $2.042$ & $2.032$ & $2.023$ & $2.014$ & $2.007$ \\
		$14$ & $0.990$ & $3.864$ & $3.800$ & $3.745$ & $3.698$ & $3.656$ & $3.619$ & $3.586$ & $3.556$ & $3.529$ & $3.505$ \\
		& $0.975$ & $3.095$ & $3.050$ & $3.012$ & $2.979$ & $2.949$ & $2.923$ & $2.900$ & $2.879$ & $2.861$ & $2.844$ \\
		& $0.950$ & $2.565$ & $2.534$ & $2.507$ & $2.484$ & $2.463$ & $2.445$ & $2.428$ & $2.413$ & $2.400$ & $2.388$ \\
		& $0.900$ & $2.073$ & $2.054$ & $2.037$ & $2.022$ & $2.010$ & $1.998$ & $1.988$ & $1.978$ & $1.970$ & $1.962$ \\
		$15$ & $0.990$ & $3.730$ & $3.666$ & $3.612$ & $3.564$ & $3.522$ & $3.485$ & $3.452$ & $3.423$ & $3.396$ & $3.372$ \\
		& $0.975$ & $3.008$ & $2.963$ & $2.925$ & $2.891$ & $2.862$ & $2.836$ & $2.813$ & $2.792$ & $2.773$ & $2.756$ \\
		& $0.950$ & $2.507$ & $2.475$ & $2.448$ & $2.424$ & $2.403$ & $2.385$ & $2.368$ & $2.353$ & $2.340$ & $2.328$ \\
		& $0.900$ & $2.037$ & $2.017$ & $2.000$ & $1.985$ & $1.972$ & $1.961$ & $1.950$ & $1.941$ & $1.932$ & $1.924$ \\
		$16$ & $0.990$ & $3.616$ & $3.553$ & $3.498$ & $3.451$ & $3.409$ & $3.372$ & $3.339$ & $3.310$ & $3.283$ & $3.259$ \\
		& $0.975$ & $2.934$ & $2.889$ & $2.851$ & $2.817$ & $2.788$ & $2.761$ & $2.738$ & $2.717$ & $2.698$ & $2.681$ \\
		& $0.950$ & $2.456$ & $2.425$ & $2.397$ & $2.373$ & $2.352$ & $2.333$ & $2.317$ & $2.302$ & $2.288$ & $2.276$ \\
		& $0.900$ & $2.005$ & $1.985$ & $1.968$ & $1.953$ & $1.940$ & $1.928$ & $1.917$ & $1.908$ & $1.899$ & $1.891$ \\
		$17$ & $0.990$ & $3.518$ & $3.455$ & $3.401$ & $3.353$ & $3.312$ & $3.275$ & $3.242$ & $3.212$ & $3.186$ & $3.162$ \\
		& $0.975$ & $2.870$ & $2.825$ & $2.786$ & $2.753$ & $2.723$ & $2.697$ & $2.673$ & $2.652$ & $2.633$ & $2.616$ \\
		& $0.950$ & $2.413$ & $2.381$ & $2.353$ & $2.329$ & $2.308$ & $2.289$ & $2.272$ & $2.257$ & $2.243$ & $2.230$ \\
		& $0.900$ & $1.978$ & $1.958$ & $1.940$ & $1.925$ & $1.912$ & $1.900$ & $1.889$ & $1.879$ & $1.870$ & $1.862$ \\
		$18$ & $0.990$ & $3.434$ & $3.371$ & $3.316$ & $3.269$ & $3.227$ & $3.190$ & $3.158$ & $3.128$ & $3.101$ & $3.077$ \\
		& $0.975$ & $2.814$ & $2.769$ & $2.730$ & $2.696$ & $2.667$ & $2.640$ & $2.617$ & $2.596$ & $2.576$ & $2.559$ \\
		& $0.950$ & $2.374$ & $2.342$ & $2.314$ & $2.290$ & $2.269$ & $2.250$ & $2.233$ & $2.217$ & $2.203$ & $2.191$ \\
		& $0.900$ & $1.954$ & $1.933$ & $1.916$ & $1.900$ & $1.887$ & $1.875$ & $1.864$ & $1.854$ & $1.845$ & $1.837$ \\
		$19$ & $0.990$ & $3.360$ & $3.297$ & $3.242$ & $3.195$ & $3.153$ & $3.116$ & $3.084$ & $3.054$ & $3.027$ & $3.003$ \\
		& $0.975$ & $2.765$ & $2.720$ & $2.681$ & $2.647$ & $2.617$ & $2.591$ & $2.567$ & $2.546$ & $2.526$ & $2.509$ \\
		& $0.950$ & $2.340$ & $2.308$ & $2.280$ & $2.256$ & $2.234$ & $2.215$ & $2.198$ & $2.182$ & $2.168$ & $2.155$ \\
		& $0.900$ & $1.932$ & $1.912$ & $1.894$ & $1.878$ & $1.865$ & $1.852$ & $1.841$ & $1.831$ & $1.822$ & $1.814$ \\
		$20$ & $0.990$ & $3.294$ & $3.231$ & $3.177$ & $3.130$ & $3.088$ & $3.051$ & $3.018$ & $2.989$ & $2.962$ & $2.938$ \\
		& $0.975$ & $2.721$ & $2.676$ & $2.637$ & $2.603$ & $2.573$ & $2.547$ & $2.523$ & $2.501$ & $2.482$ & $2.464$ \\
		& $0.950$ & $2.310$ & $2.278$ & $2.250$ & $2.225$ & $2.203$ & $2.184$ & $2.167$ & $2.151$ & $2.137$ & $2.124$ \\
		& $0.900$ & $1.913$ & $1.892$ & $1.875$ & $1.859$ & $1.845$ & $1.833$ & $1.821$ & $1.811$ & $1.802$ & $1.794$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$11-20$, m:$11-20$}
\end{table}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $25$ & $30$ & $40$ & $50$ & $60$ & $80$ & $100$ & $200$ & $500$ & $\infty$\\
		\hline
		$11$ & $0.990$ & $4.005$ & $3.941$ & $3.860$ & $3.810$ & $3.776$ & $3.734$ & $3.708$ & $3.656$ & $3.624$ & $3.603$ \\
		& $0.975$ & $3.162$ & $3.118$ & $3.061$ & $3.027$ & $3.004$ & $2.974$ & $2.956$ & $2.920$ & $2.898$ & $2.883$ \\
		& $0.950$ & $2.601$ & $2.570$ & $2.531$ & $2.507$ & $2.490$ & $2.469$ & $2.457$ & $2.431$ & $2.415$ & $2.405$ \\
		& $0.900$ & $2.095$ & $2.076$ & $2.052$ & $2.036$ & $2.026$ & $2.013$ & $2.005$ & $1.989$ & $1.979$ & $1.972$ \\
		$12$ & $0.990$ & $3.765$ & $3.701$ & $3.619$ & $3.569$ & $3.535$ & $3.493$ & $3.467$ & $3.414$ & $3.382$ & $3.361$ \\
		& $0.975$ & $3.008$ & $2.963$ & $2.906$ & $2.871$ & $2.848$ & $2.818$ & $2.800$ & $2.763$ & $2.740$ & $2.725$ \\
		& $0.950$ & $2.498$ & $2.466$ & $2.426$ & $2.401$ & $2.384$ & $2.363$ & $2.350$ & $2.323$ & $2.307$ & $2.296$ \\
		& $0.900$ & $2.031$ & $2.011$ & $1.986$ & $1.970$ & $1.960$ & $1.946$ & $1.938$ & $1.921$ & $1.911$ & $1.904$ \\
		$13$ & $0.990$ & $3.571$ & $3.507$ & $3.425$ & $3.375$ & $3.341$ & $3.298$ & $3.272$ & $3.219$ & $3.187$ & $3.165$ \\
		& $0.975$ & $2.882$ & $2.837$ & $2.780$ & $2.744$ & $2.720$ & $2.690$ & $2.671$ & $2.634$ & $2.611$ & $2.596$ \\
		& $0.950$ & $2.412$ & $2.380$ & $2.339$ & $2.314$ & $2.297$ & $2.275$ & $2.261$ & $2.234$ & $2.218$ & $2.206$ \\
		& $0.900$ & $1.978$ & $1.958$ & $1.931$ & $1.915$ & $1.904$ & $1.890$ & $1.882$ & $1.864$ & $1.853$ & $1.846$ \\
		$14$ & $0.990$ & $3.412$ & $3.348$ & $3.266$ & $3.215$ & $3.181$ & $3.138$ & $3.112$ & $3.059$ & $3.026$ & $3.004$ \\
		& $0.975$ & $2.778$ & $2.732$ & $2.674$ & $2.638$ & $2.614$ & $2.583$ & $2.565$ & $2.526$ & $2.503$ & $2.487$ \\
		& $0.950$ & $2.341$ & $2.308$ & $2.266$ & $2.241$ & $2.223$ & $2.201$ & $2.187$ & $2.159$ & $2.142$ & $2.131$ \\
		& $0.900$ & $1.933$ & $1.912$ & $1.885$ & $1.869$ & $1.857$ & $1.843$ & $1.834$ & $1.816$ & $1.805$ & $1.797$ \\
		$15$ & $0.990$ & $3.278$ & $3.214$ & $3.132$ & $3.081$ & $3.047$ & $3.004$ & $2.977$ & $2.923$ & $2.891$ & $2.869$ \\
		& $0.975$ & $2.689$ & $2.644$ & $2.585$ & $2.549$ & $2.524$ & $2.493$ & $2.474$ & $2.435$ & $2.411$ & $2.395$ \\
		& $0.950$ & $2.280$ & $2.247$ & $2.204$ & $2.178$ & $2.160$ & $2.137$ & $2.123$ & $2.095$ & $2.078$ & $2.066$ \\
		& $0.900$ & $1.894$ & $1.873$ & $1.845$ & $1.828$ & $1.817$ & $1.802$ & $1.793$ & $1.774$ & $1.763$ & $1.755$ \\
		$16$ & $0.990$ & $3.165$ & $3.101$ & $3.018$ & $2.967$ & $2.933$ & $2.889$ & $2.863$ & $2.808$ & $2.775$ & $2.753$ \\
		& $0.975$ & $2.614$ & $2.568$ & $2.509$ & $2.472$ & $2.447$ & $2.415$ & $2.396$ & $2.357$ & $2.333$ & $2.316$ \\
		& $0.950$ & $2.227$ & $2.194$ & $2.151$ & $2.124$ & $2.106$ & $2.083$ & $2.068$ & $2.039$ & $2.022$ & $2.010$ \\
		& $0.900$ & $1.860$ & $1.839$ & $1.811$ & $1.793$ & $1.782$ & $1.766$ & $1.757$ & $1.738$ & $1.726$ & $1.718$ \\
		$17$ & $0.990$ & $3.068$ & $3.003$ & $2.920$ & $2.869$ & $2.835$ & $2.791$ & $2.764$ & $2.709$ & $2.676$ & $2.653$ \\
		& $0.975$ & $2.548$ & $2.502$ & $2.442$ & $2.405$ & $2.380$ & $2.348$ & $2.329$ & $2.289$ & $2.264$ & $2.248$ \\
		& $0.950$ & $2.181$ & $2.148$ & $2.104$ & $2.077$ & $2.058$ & $2.035$ & $2.020$ & $1.991$ & $1.973$ & $1.960$ \\
		& $0.900$ & $1.831$ & $1.809$ & $1.781$ & $1.763$ & $1.751$ & $1.735$ & $1.726$ & $1.706$ & $1.694$ & $1.686$ \\
		$18$ & $0.990$ & $2.983$ & $2.919$ & $2.835$ & $2.784$ & $2.749$ & $2.705$ & $2.678$ & $2.623$ & $2.589$ & $2.566$ \\
		& $0.975$ & $2.491$ & $2.445$ & $2.384$ & $2.347$ & $2.321$ & $2.289$ & $2.269$ & $2.229$ & $2.204$ & $2.187$ \\
		& $0.950$ & $2.141$ & $2.107$ & $2.063$ & $2.035$ & $2.017$ & $1.993$ & $1.978$ & $1.948$ & $1.929$ & $1.917$ \\
		& $0.900$ & $1.805$ & $1.783$ & $1.754$ & $1.736$ & $1.723$ & $1.707$ & $1.698$ & $1.678$ & $1.665$ & $1.657$ \\
		$19$ & $0.990$ & $2.909$ & $2.844$ & $2.761$ & $2.709$ & $2.674$ & $2.630$ & $2.602$ & $2.547$ & $2.512$ & $2.489$ \\
		& $0.975$ & $2.441$ & $2.394$ & $2.333$ & $2.295$ & $2.270$ & $2.237$ & $2.217$ & $2.176$ & $2.150$ & $2.133$ \\
		& $0.950$ & $2.106$ & $2.071$ & $2.026$ & $1.999$ & $1.980$ & $1.955$ & $1.940$ & $1.910$ & $1.891$ & $1.878$ \\
		& $0.900$ & $1.782$ & $1.759$ & $1.730$ & $1.711$ & $1.699$ & $1.683$ & $1.673$ & $1.652$ & $1.639$ & $1.631$ \\
		$20$ & $0.990$ & $2.843$ & $2.778$ & $2.695$ & $2.643$ & $2.608$ & $2.563$ & $2.535$ & $2.479$ & $2.445$ & $2.421$ \\
		& $0.975$ & $2.396$ & $2.349$ & $2.287$ & $2.249$ & $2.223$ & $2.190$ & $2.170$ & $2.128$ & $2.103$ & $2.085$ \\
		& $0.950$ & $2.074$ & $2.039$ & $1.994$ & $1.966$ & $1.946$ & $1.922$ & $1.907$ & $1.875$ & $1.856$ & $1.843$ \\
		& $0.900$ & $1.761$ & $1.738$ & $1.708$ & $1.690$ & $1.677$ & $1.660$ & $1.650$ & $1.629$ & $1.616$ & $1.607$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$11-20$, m:$25-\infty$}
\end{table}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $1$ & $2$ & $3$ & $4$ & $5$ & $6$ & $7$ & $8$ & $9$ & $10$\\
		\hline
		$25$ & $0.990$ & $7.770$ & $5.568$ & $4.675$ & $4.177$ & $3.855$ & $3.627$ & $3.457$ & $3.324$ & $3.217$ & $3.129$ \\
		& $0.975$ & $5.686$ & $4.291$ & $3.694$ & $3.353$ & $3.129$ & $2.969$ & $2.848$ & $2.753$ & $2.677$ & $2.613$ \\
		& $0.950$ & $4.242$ & $3.385$ & $2.991$ & $2.759$ & $2.603$ & $2.490$ & $2.405$ & $2.337$ & $2.282$ & $2.236$ \\
		& $0.900$ & $2.918$ & $2.528$ & $2.317$ & $2.184$ & $2.092$ & $2.024$ & $1.971$ & $1.929$ & $1.895$ & $1.866$ \\
		$30$ & $0.990$ & $7.562$ & $5.390$ & $4.510$ & $4.018$ & $3.699$ & $3.473$ & $3.305$ & $3.173$ & $3.067$ & $2.979$ \\
		& $0.975$ & $5.568$ & $4.182$ & $3.589$ & $3.250$ & $3.026$ & $2.867$ & $2.746$ & $2.651$ & $2.575$ & $2.511$ \\
		& $0.950$ & $4.171$ & $3.316$ & $2.922$ & $2.690$ & $2.534$ & $2.421$ & $2.334$ & $2.266$ & $2.211$ & $2.165$ \\
		& $0.900$ & $2.881$ & $2.489$ & $2.276$ & $2.142$ & $2.049$ & $1.980$ & $1.927$ & $1.884$ & $1.849$ & $1.819$ \\
		$40$ & $0.990$ & $7.314$ & $5.178$ & $4.313$ & $3.828$ & $3.514$ & $3.291$ & $3.124$ & $2.993$ & $2.888$ & $2.801$ \\
		& $0.975$ & $5.424$ & $4.051$ & $3.463$ & $3.126$ & $2.904$ & $2.744$ & $2.624$ & $2.529$ & $2.452$ & $2.388$ \\
		& $0.950$ & $4.085$ & $3.232$ & $2.839$ & $2.606$ & $2.449$ & $2.336$ & $2.249$ & $2.180$ & $2.124$ & $2.077$ \\
		& $0.900$ & $2.835$ & $2.440$ & $2.226$ & $2.091$ & $1.997$ & $1.927$ & $1.873$ & $1.829$ & $1.793$ & $1.763$ \\
		$50$ & $0.990$ & $7.171$ & $5.057$ & $4.199$ & $3.720$ & $3.408$ & $3.186$ & $3.020$ & $2.890$ & $2.785$ & $2.698$ \\
		& $0.975$ & $5.340$ & $3.975$ & $3.390$ & $3.054$ & $2.833$ & $2.674$ & $2.553$ & $2.458$ & $2.381$ & $2.317$ \\
		& $0.950$ & $4.034$ & $3.183$ & $2.790$ & $2.557$ & $2.400$ & $2.286$ & $2.199$ & $2.130$ & $2.073$ & $2.026$ \\
		& $0.900$ & $2.809$ & $2.412$ & $2.197$ & $2.061$ & $1.966$ & $1.895$ & $1.840$ & $1.796$ & $1.760$ & $1.729$ \\
		$60$ & $0.990$ & $7.077$ & $4.977$ & $4.126$ & $3.649$ & $3.339$ & $3.119$ & $2.953$ & $2.823$ & $2.718$ & $2.632$ \\
		& $0.975$ & $5.286$ & $3.925$ & $3.343$ & $3.008$ & $2.786$ & $2.627$ & $2.507$ & $2.412$ & $2.334$ & $2.270$ \\
		& $0.950$ & $4.001$ & $3.150$ & $2.758$ & $2.525$ & $2.368$ & $2.254$ & $2.167$ & $2.097$ & $2.040$ & $1.993$ \\
		& $0.900$ & $2.791$ & $2.393$ & $2.177$ & $2.041$ & $1.946$ & $1.875$ & $1.819$ & $1.775$ & $1.738$ & $1.707$ \\
		$80$ & $0.990$ & $6.963$ & $4.881$ & $4.036$ & $3.563$ & $3.255$ & $3.036$ & $2.871$ & $2.742$ & $2.637$ & $2.551$ \\
		& $0.975$ & $5.218$ & $3.864$ & $3.284$ & $2.950$ & $2.730$ & $2.571$ & $2.450$ & $2.355$ & $2.277$ & $2.213$ \\
		& $0.950$ & $3.960$ & $3.111$ & $2.719$ & $2.486$ & $2.329$ & $2.214$ & $2.126$ & $2.056$ & $1.999$ & $1.951$ \\
		& $0.900$ & $2.769$ & $2.370$ & $2.154$ & $2.016$ & $1.921$ & $1.849$ & $1.793$ & $1.748$ & $1.711$ & $1.680$ \\
		$100$ & $0.990$ & $6.895$ & $4.824$ & $3.984$ & $3.513$ & $3.206$ & $2.988$ & $2.823$ & $2.694$ & $2.590$ & $2.503$ \\
		& $0.975$ & $5.179$ & $3.828$ & $3.250$ & $2.917$ & $2.696$ & $2.537$ & $2.417$ & $2.321$ & $2.244$ & $2.179$ \\
		& $0.950$ & $3.936$ & $3.087$ & $2.696$ & $2.463$ & $2.305$ & $2.191$ & $2.103$ & $2.032$ & $1.975$ & $1.927$ \\
		& $0.900$ & $2.756$ & $2.356$ & $2.139$ & $2.002$ & $1.906$ & $1.834$ & $1.778$ & $1.732$ & $1.695$ & $1.663$ \\
		$200$ & $0.990$ & $6.763$ & $4.713$ & $3.881$ & $3.414$ & $3.110$ & $2.893$ & $2.730$ & $2.601$ & $2.497$ & $2.411$ \\
		& $0.975$ & $5.100$ & $3.758$ & $3.182$ & $2.850$ & $2.630$ & $2.472$ & $2.351$ & $2.256$ & $2.178$ & $2.113$ \\
		& $0.950$ & $3.888$ & $3.041$ & $2.650$ & $2.417$ & $2.259$ & $2.144$ & $2.056$ & $1.985$ & $1.927$ & $1.878$ \\
		& $0.900$ & $2.731$ & $2.329$ & $2.111$ & $1.973$ & $1.876$ & $1.804$ & $1.747$ & $1.701$ & $1.663$ & $1.631$ \\
		$500$ & $0.990$ & $6.686$ & $4.648$ & $3.821$ & $3.357$ & $3.054$ & $2.838$ & $2.675$ & $2.547$ & $2.443$ & $2.356$ \\
		& $0.975$ & $5.054$ & $3.716$ & $3.142$ & $2.811$ & $2.592$ & $2.434$ & $2.313$ & $2.217$ & $2.139$ & $2.074$ \\
		& $0.950$ & $3.860$ & $3.014$ & $2.623$ & $2.390$ & $2.232$ & $2.117$ & $2.028$ & $1.957$ & $1.899$ & $1.850$ \\
		& $0.900$ & $2.716$ & $2.313$ & $2.095$ & $1.956$ & $1.859$ & $1.786$ & $1.729$ & $1.683$ & $1.644$ & $1.612$ \\
		$\infty$ & $0.990$ & $6.635$ & $4.605$ & $3.782$ & $3.319$ & $3.017$ & $2.802$ & $2.639$ & $2.511$ & $2.407$ & $2.321$ \\
		& $0.975$ & $5.024$ & $3.689$ & $3.116$ & $2.786$ & $2.567$ & $2.408$ & $2.288$ & $2.192$ & $2.114$ & $2.048$ \\
		& $0.950$ & $3.841$ & $2.996$ & $2.605$ & $2.372$ & $2.214$ & $2.099$ & $2.010$ & $1.938$ & $1.880$ & $1.831$ \\
		& $0.900$ & $2.706$ & $2.303$ & $2.084$ & $1.945$ & $1.847$ & $1.774$ & $1.717$ & $1.670$ & $1.632$ & $1.599$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$25-\infty$, m:$1-10$}
\end{table}

\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $11$ & $12$ & $13$ & $14$ & $15$ & $16$ & $17$ & $18$ & $19$ & $20$\\
		\hline
		$25$ & $0.990$ & $3.056$ & $2.993$ & $2.939$ & $2.892$ & $2.850$ & $2.813$ & $2.780$ & $2.751$ & $2.724$ & $2.699$ \\
		& $0.975$ & $2.560$ & $2.515$ & $2.476$ & $2.441$ & $2.411$ & $2.384$ & $2.360$ & $2.338$ & $2.318$ & $2.300$ \\
		& $0.950$ & $2.198$ & $2.165$ & $2.136$ & $2.111$ & $2.089$ & $2.069$ & $2.051$ & $2.035$ & $2.021$ & $2.007$ \\
		& $0.900$ & $1.841$ & $1.820$ & $1.802$ & $1.785$ & $1.771$ & $1.758$ & $1.746$ & $1.736$ & $1.726$ & $1.718$ \\
		$30$ & $0.990$ & $2.906$ & $2.843$ & $2.789$ & $2.742$ & $2.700$ & $2.663$ & $2.630$ & $2.600$ & $2.573$ & $2.549$ \\
		& $0.975$ & $2.458$ & $2.412$ & $2.372$ & $2.338$ & $2.307$ & $2.280$ & $2.255$ & $2.233$ & $2.213$ & $2.195$ \\
		& $0.950$ & $2.126$ & $2.092$ & $2.063$ & $2.037$ & $2.015$ & $1.995$ & $1.976$ & $1.960$ & $1.945$ & $1.932$ \\
		& $0.900$ & $1.794$ & $1.773$ & $1.754$ & $1.737$ & $1.722$ & $1.709$ & $1.697$ & $1.686$ & $1.676$ & $1.667$ \\
		$40$ & $0.990$ & $2.727$ & $2.665$ & $2.611$ & $2.563$ & $2.522$ & $2.484$ & $2.451$ & $2.421$ & $2.394$ & $2.369$ \\
		& $0.975$ & $2.334$ & $2.288$ & $2.248$ & $2.213$ & $2.182$ & $2.154$ & $2.129$ & $2.107$ & $2.086$ & $2.068$ \\
		& $0.950$ & $2.038$ & $2.003$ & $1.974$ & $1.948$ & $1.924$ & $1.904$ & $1.885$ & $1.868$ & $1.853$ & $1.839$ \\
		& $0.900$ & $1.737$ & $1.715$ & $1.695$ & $1.678$ & $1.662$ & $1.649$ & $1.636$ & $1.625$ & $1.615$ & $1.605$ \\
		$50$ & $0.990$ & $2.625$ & $2.563$ & $2.508$ & $2.461$ & $2.419$ & $2.382$ & $2.348$ & $2.318$ & $2.290$ & $2.265$ \\
		& $0.975$ & $2.263$ & $2.216$ & $2.176$ & $2.140$ & $2.109$ & $2.081$ & $2.056$ & $2.033$ & $2.012$ & $1.993$ \\
		& $0.950$ & $1.986$ & $1.952$ & $1.921$ & $1.895$ & $1.871$ & $1.850$ & $1.831$ & $1.814$ & $1.798$ & $1.784$ \\
		& $0.900$ & $1.703$ & $1.680$ & $1.660$ & $1.643$ & $1.627$ & $1.613$ & $1.600$ & $1.588$ & $1.578$ & $1.568$ \\
		$60$ & $0.990$ & $2.559$ & $2.496$ & $2.442$ & $2.394$ & $2.352$ & $2.315$ & $2.281$ & $2.251$ & $2.223$ & $2.198$ \\
		& $0.975$ & $2.216$ & $2.169$ & $2.129$ & $2.093$ & $2.061$ & $2.033$ & $2.008$ & $1.985$ & $1.964$ & $1.944$ \\
		& $0.950$ & $1.952$ & $1.917$ & $1.887$ & $1.860$ & $1.836$ & $1.815$ & $1.796$ & $1.778$ & $1.763$ & $1.748$ \\
		& $0.900$ & $1.680$ & $1.657$ & $1.637$ & $1.619$ & $1.603$ & $1.589$ & $1.576$ & $1.564$ & $1.553$ & $1.543$ \\
		$80$ & $0.990$ & $2.478$ & $2.415$ & $2.361$ & $2.313$ & $2.271$ & $2.233$ & $2.199$ & $2.169$ & $2.141$ & $2.115$ \\
		& $0.975$ & $2.158$ & $2.111$ & $2.071$ & $2.035$ & $2.003$ & $1.974$ & $1.948$ & $1.925$ & $1.904$ & $1.884$ \\
		& $0.950$ & $1.910$ & $1.875$ & $1.845$ & $1.817$ & $1.793$ & $1.772$ & $1.752$ & $1.734$ & $1.718$ & $1.703$ \\
		& $0.900$ & $1.653$ & $1.629$ & $1.609$ & $1.590$ & $1.574$ & $1.559$ & $1.546$ & $1.534$ & $1.523$ & $1.513$ \\
		$100$ & $0.990$ & $2.430$ & $2.368$ & $2.313$ & $2.265$ & $2.223$ & $2.185$ & $2.151$ & $2.120$ & $2.092$ & $2.067$ \\
		& $0.975$ & $2.124$ & $2.077$ & $2.036$ & $2.000$ & $1.968$ & $1.939$ & $1.913$ & $1.890$ & $1.868$ & $1.849$ \\
		& $0.950$ & $1.886$ & $1.850$ & $1.819$ & $1.792$ & $1.768$ & $1.746$ & $1.726$ & $1.708$ & $1.691$ & $1.676$ \\
		& $0.900$ & $1.636$ & $1.612$ & $1.592$ & $1.573$ & $1.557$ & $1.542$ & $1.528$ & $1.516$ & $1.505$ & $1.494$ \\
		$200$ & $0.990$ & $2.338$ & $2.275$ & $2.220$ & $2.172$ & $2.129$ & $2.091$ & $2.057$ & $2.026$ & $1.997$ & $1.971$ \\
		& $0.975$ & $2.058$ & $2.010$ & $1.969$ & $1.932$ & $1.900$ & $1.870$ & $1.844$ & $1.820$ & $1.798$ & $1.778$ \\
		& $0.950$ & $1.837$ & $1.801$ & $1.769$ & $1.742$ & $1.717$ & $1.694$ & $1.674$ & $1.656$ & $1.639$ & $1.623$ \\
		& $0.900$ & $1.603$ & $1.579$ & $1.558$ & $1.539$ & $1.522$ & $1.507$ & $1.493$ & $1.480$ & $1.468$ & $1.458$ \\
		$500$ & $0.990$ & $2.283$ & $2.220$ & $2.166$ & $2.117$ & $2.075$ & $2.036$ & $2.002$ & $1.970$ & $1.942$ & $1.915$ \\
		& $0.975$ & $2.019$ & $1.971$ & $1.929$ & $1.892$ & $1.859$ & $1.830$ & $1.803$ & $1.779$ & $1.757$ & $1.736$ \\
		& $0.950$ & $1.808$ & $1.772$ & $1.740$ & $1.712$ & $1.686$ & $1.664$ & $1.643$ & $1.625$ & $1.607$ & $1.592$ \\
		& $0.900$ & $1.583$ & $1.559$ & $1.537$ & $1.518$ & $1.501$ & $1.485$ & $1.471$ & $1.458$ & $1.446$ & $1.435$ \\
		$\infty$ & $0.990$ & $2.248$ & $2.185$ & $2.130$ & $2.082$ & $2.039$ & $2.000$ & $1.965$ & $1.934$ & $1.905$ & $1.878$ \\
		& $0.975$ & $1.993$ & $1.945$ & $1.903$ & $1.866$ & $1.833$ & $1.803$ & $1.776$ & $1.751$ & $1.729$ & $1.708$ \\
		& $0.950$ & $1.789$ & $1.752$ & $1.720$ & $1.692$ & $1.666$ & $1.644$ & $1.623$ & $1.604$ & $1.587$ & $1.571$ \\
		& $0.900$ & $1.570$ & $1.546$ & $1.524$ & $1.505$ & $1.487$ & $1.471$ & $1.457$ & $1.444$ & $1.432$ & $1.421$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$25-\infty$, m:$11-20$}
\end{table}
\begin{table}[H]
	\begin{tabularx}{\linewidth}{c | c | c c c c c c c c c c}
		& & \multicolumn{10}{c}{m}\\
		\cline{3-12}
		n & q & $25$ & $30$ & $40$ & $50$ & $60$ & $80$ & $100$ & $200$ & $500$ & $\infty$\\
		\hline
		$25$ & $0.990$ & $2.604$ & $2.538$ & $2.453$ & $2.400$ & $2.364$ & $2.317$ & $2.289$ & $2.230$ & $2.194$ & $2.170$ \\
		& $0.975$ & $2.230$ & $2.182$ & $2.118$ & $2.079$ & $2.052$ & $2.017$ & $1.996$ & $1.952$ & $1.924$ & $1.906$ \\
		& $0.950$ & $1.955$ & $1.919$ & $1.872$ & $1.842$ & $1.822$ & $1.796$ & $1.779$ & $1.746$ & $1.725$ & $1.711$ \\
		& $0.900$ & $1.683$ & $1.659$ & $1.627$ & $1.607$ & $1.593$ & $1.576$ & $1.565$ & $1.542$ & $1.527$ & $1.518$ \\
		$30$ & $0.990$ & $2.453$ & $2.386$ & $2.299$ & $2.245$ & $2.208$ & $2.160$ & $2.131$ & $2.070$ & $2.032$ & $2.006$ \\
		& $0.975$ & $2.124$ & $2.074$ & $2.009$ & $1.968$ & $1.940$ & $1.904$ & $1.882$ & $1.835$ & $1.806$ & $1.787$ \\
		& $0.950$ & $1.878$ & $1.841$ & $1.792$ & $1.761$ & $1.740$ & $1.712$ & $1.695$ & $1.660$ & $1.637$ & $1.622$ \\
		& $0.900$ & $1.632$ & $1.606$ & $1.573$ & $1.552$ & $1.538$ & $1.519$ & $1.507$ & $1.482$ & $1.467$ & $1.456$ \\
		$40$ & $0.990$ & $2.271$ & $2.203$ & $2.114$ & $2.058$ & $2.019$ & $1.969$ & $1.938$ & $1.874$ & $1.833$ & $1.805$ \\
		& $0.975$ & $1.994$ & $1.943$ & $1.875$ & $1.832$ & $1.803$ & $1.764$ & $1.741$ & $1.691$ & $1.659$ & $1.637$ \\
		& $0.950$ & $1.783$ & $1.744$ & $1.693$ & $1.660$ & $1.637$ & $1.608$ & $1.589$ & $1.551$ & $1.526$ & $1.509$ \\
		& $0.900$ & $1.568$ & $1.541$ & $1.506$ & $1.483$ & $1.467$ & $1.447$ & $1.434$ & $1.406$ & $1.389$ & $1.377$ \\
		$50$ & $0.990$ & $2.167$ & $2.098$ & $2.007$ & $1.949$ & $1.909$ & $1.857$ & $1.825$ & $1.757$ & $1.713$ & $1.683$ \\
		& $0.975$ & $1.919$ & $1.866$ & $1.796$ & $1.752$ & $1.721$ & $1.681$ & $1.656$ & $1.603$ & $1.569$ & $1.545$ \\
		& $0.950$ & $1.727$ & $1.687$ & $1.634$ & $1.599$ & $1.576$ & $1.544$ & $1.525$ & $1.484$ & $1.457$ & $1.438$ \\
		& $0.900$ & $1.529$ & $1.502$ & $1.465$ & $1.441$ & $1.424$ & $1.402$ & $1.388$ & $1.359$ & $1.340$ & $1.327$ \\
		$60$ & $0.990$ & $2.098$ & $2.028$ & $1.936$ & $1.877$ & $1.836$ & $1.783$ & $1.749$ & $1.678$ & $1.633$ & $1.601$ \\
		& $0.975$ & $1.869$ & $1.815$ & $1.744$ & $1.699$ & $1.667$ & $1.625$ & $1.599$ & $1.543$ & $1.507$ & $1.482$ \\
		& $0.950$ & $1.690$ & $1.649$ & $1.594$ & $1.559$ & $1.534$ & $1.502$ & $1.481$ & $1.438$ & $1.409$ & $1.389$ \\
		& $0.900$ & $1.504$ & $1.476$ & $1.437$ & $1.413$ & $1.395$ & $1.372$ & $1.358$ & $1.326$ & $1.306$ & $1.292$ \\
		$80$ & $0.990$ & $2.015$ & $1.944$ & $1.849$ & $1.788$ & $1.746$ & $1.690$ & $1.655$ & $1.579$ & $1.530$ & $1.494$ \\
		& $0.975$ & $1.807$ & $1.752$ & $1.679$ & $1.632$ & $1.599$ & $1.555$ & $1.527$ & $1.467$ & $1.428$ & $1.400$ \\
		& $0.950$ & $1.644$ & $1.602$ & $1.545$ & $1.508$ & $1.482$ & $1.448$ & $1.426$ & $1.379$ & $1.347$ & $1.325$ \\
		& $0.900$ & $1.472$ & $1.443$ & $1.403$ & $1.377$ & $1.358$ & $1.334$ & $1.318$ & $1.284$ & $1.261$ & $1.245$ \\
		$100$ & $0.990$ & $1.965$ & $1.893$ & $1.797$ & $1.735$ & $1.692$ & $1.634$ & $1.598$ & $1.518$ & $1.466$ & $1.427$ \\
		& $0.975$ & $1.770$ & $1.715$ & $1.640$ & $1.592$ & $1.558$ & $1.512$ & $1.483$ & $1.420$ & $1.378$ & $1.347$ \\
		& $0.950$ & $1.616$ & $1.573$ & $1.515$ & $1.477$ & $1.450$ & $1.415$ & $1.392$ & $1.342$ & $1.308$ & $1.283$ \\
		& $0.900$ & $1.453$ & $1.423$ & $1.382$ & $1.355$ & $1.336$ & $1.310$ & $1.293$ & $1.257$ & $1.232$ & $1.214$ \\
		$200$ & $0.990$ & $1.868$ & $1.794$ & $1.694$ & $1.629$ & $1.583$ & $1.521$ & $1.481$ & $1.391$ & $1.328$ & $1.279$ \\
		& $0.975$ & $1.698$ & $1.640$ & $1.562$ & $1.511$ & $1.474$ & $1.425$ & $1.393$ & $1.320$ & $1.269$ & $1.229$ \\
		& $0.950$ & $1.561$ & $1.516$ & $1.455$ & $1.415$ & $1.386$ & $1.346$ & $1.321$ & $1.263$ & $1.221$ & $1.189$ \\
		& $0.900$ & $1.414$ & $1.383$ & $1.339$ & $1.310$ & $1.289$ & $1.261$ & $1.242$ & $1.199$ & $1.168$ & $1.144$ \\
		$500$ & $0.990$ & $1.810$ & $1.735$ & $1.633$ & $1.566$ & $1.517$ & $1.452$ & $1.408$ & $1.308$ & $1.232$ & $1.165$ \\
		& $0.975$ & $1.655$ & $1.596$ & $1.515$ & $1.462$ & $1.423$ & $1.370$ & $1.336$ & $1.254$ & $1.192$ & $1.137$ \\
		& $0.950$ & $1.528$ & $1.482$ & $1.419$ & $1.376$ & $1.345$ & $1.303$ & $1.275$ & $1.210$ & $1.159$ & $1.113$ \\
		& $0.900$ & $1.391$ & $1.358$ & $1.313$ & $1.282$ & $1.260$ & $1.229$ & $1.209$ & $1.160$ & $1.122$ & $1.087$ \\
		$\infty$ & $0.990$ & $1.773$ & $1.696$ & $1.592$ & $1.523$ & $1.473$ & $1.404$ & $1.358$ & $1.247$ & $1.153$ & $1.000$ \\
		& $0.975$ & $1.626$ & $1.566$ & $1.484$ & $1.428$ & $1.388$ & $1.333$ & $1.296$ & $1.205$ & $1.128$ & $1.000$ \\
		& $0.950$ & $1.506$ & $1.459$ & $1.394$ & $1.350$ & $1.318$ & $1.274$ & $1.243$ & $1.170$ & $1.106$ & $1.000$ \\
		& $0.900$ & $1.375$ & $1.342$ & $1.295$ & $1.263$ & $1.240$ & $1.207$ & $1.185$ & $1.130$ & $1.082$ & $1.000$
	\end{tabularx}
	\caption{Critical values of the F-distribution n:$25-\infty$, m:$25-\infty$}
	\label{tbl:FdistributionEnd}
\end{table}

\end{document}
